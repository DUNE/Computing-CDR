\label{ch:sam:catalog}

\hideme{Decide what to do with the catalog section}
\section{Existing SAM data management system}

The existing DUNE data catalog \dword{sam} was originally designed for the D0 and CDF high energy physics experiments at Fermilab.  It is now used by most of the Intensity Frontier experiments at Fermilab. 

The most important objects cataloged in \dword{sam} are individual files and collections of files called
datasets.

Data files themselves are not stored in \dword{sam}, their metadata is, and that metadata allows you to search for and find the actual physical files.

\subsection{General considerations}

\dword{sam} was designed to ensure that large scale data-processing was done accurately and completely  which led to  high standards of reproducibility and documentation in data analysis.

For example, the time of the original design, the main storage medium was 8mm tapes using consumer-grade drives.  Drive and tape failure rates were $> 1$\%.  Several \dword{sam} design concepts, notably luminosity blocks and parentage tracking, were introduced to allow accurate tracking of files and their associated normalization in a high error-rate environment. 

\section{Requirements in replacing SAM functionality}

The \dword{sam} system has seen us through the first protoDUNE runs but going forward, a replacement is needed. 

% Goals of \dword{sam} replacement : Generalized from events and files to objects and collections

% Collections could be long trigger records, data from different detectors over a particular time period, …

 \dword{sam} is almost completely file based and duplicates run-level information.  The replacement should support use of run or trigger record level information as appropriate. 
For example,  DUNE trigger records may span multiple files. The new model needs to generalize from `events' and files to data objects and collections across multiple scales. 

Here we list the existing functions of the \dword{sam} system. 

\begin{enumerate}

\item	Describe the contents of individual files in a searchable manner. 

\item	Create and document data collections `datasets' to allow later retrieval based on data characteristics.

 \item	Track object and collection parentage and describe processing transformations to document the full provenance of any data object and ensure accurate normalization

% \item	Group  objects and collection into larger “datasets” based on their characteristics

 \item	Store the physical  location of objects.

 \item	Track the processing of collections to allow reprocessing on failure and avoid double processing.

 \item	Provide methods for delivering and tracking collections in multi-process jobs.

 \item	Preserve data about processing/storage operations for debugging/reporting.

\end{enumerate}
The first 3 goals relate to content and characteristics while the last 4 relate to data storage and processing tools. We propose to break the existing system up in to functional sub-units specializing in cataloging, storage and delivery. 

\begin{description}
\item{metacat}  The catalog function will be replaced by a combination of a file metadata catalog \dword{metacat} and a run configuration database that stores run level information.
\item{rucio}  The file location function is replaced by \dword{rucio}\cite{Barisits:2019fyl}, the file tracking system originally developed by the ATLAS collaboration. Rucio provides a location catalog for files and rule-based transfers between sites. 
\item{data dispatcher} The existing system supports a station and \dword{samproject} ecosystem where datasets are submitted for processing as \dwords{samproject}.  The \dword{samproject} is a server process that contains a list of files based on a dataset description  and, when asked, delivers location information for the next file in the list.  Request, delivery and processing are tracked and recorded. This allows resubmission on failure. A website allows users to see the details of file delivery. 
\item{FTS} 

\end{description}



% \begin{enumerate}

% \item 	The current \dword{sam} implementation uses the file as the basic unit of information.  Metadata is associated with the file name.  Filenames much be unique in the system. This prevents duplication of data in a sample, as a second copy cannot be cataloged. %This makes renaming a file very unwise. A very common practice is to include some of the metadata in the filename, both to make it easier to identify and to ensure uniqueness.    

% A \dword{sam} replacement needs to move away from physical files as the unit of data collection.  Data collections should be hierarchical with collections able to contain other collections.

% Metadata can be associated with an object, a time period (runs), a processing step or a configuration (runs). Objects/data collections need pointers to the appropriate configuration. 

% \item	Metadata for a file can include file locations but does not have to. A file can have no location at all, or many. When you move or remove a file with an associated \dword{sam} location, you need to update the location information.
% Rucio will handle this location but mapping to data collections needs to be clear and robust. 

 

% \item	Files which are stored on disk or tape are expected to have appropriate file sizes and checksums. One can have duplicate instances of a file in different locations, but they must all be identical.  


% \item	Files can have parents and children and, effectively birth certificates that can tell you how they were made.  An example would be a set of raw data files RAWnnn processed with code X to produce a single reconstructed file RECO.  One can tell \dword{sam} that RAWnnn are the parents of RECO processed with version x of code X. If one later finds another RAWnnn file that was missed in processing, \dword{sam} can tell you it has not been processed yet with X (i.e., it has no children associated with version x of X) and you can then choose to process that file. This use case often occurs when a production job fails without reporting back or succeeds but the copy back or catalog action fails. 

% This provenance information is important to retain at the object and collection level. 

% The D0 experiment required that all official processing going into \dword{sam} be done with tagged releases and fixed parameter sets to increase reproducibility and the tags for that information were included in the metadata. Calibration databases were harder to timestamp so some variability was still possible if calibrations were updated.

\subsection{Catalog features}

\subsubsection{Data description values and parameters}

 \dword{sam} supports several types of data description fields:

% Values are common across all implementations and optimized for fast retrieval.  High level concepts such as data tier and detector type are example. 

% Parameters are defined by the experiment and are extensible.

\dword{sam}  Generic `values' such as data\_tier, run\_type and file\_size  are common to almost all  HEP experiments and are optimized for efficient queries.  \dword{sam} also allows definition of free-form `parameters' as they are needed.  This allows the schema to be modified easily as needs arise.  \dword{metacat} extends these concepts to make them easier to search and maintain. 

% a.	We need optimized types for those most frequently used

% b.	We need expandable but enumerated types for important types.  Example would be detector configurations, which need to be added per experiment but should have a small set of valid values and not allow free form entries. 

% c.	We also need free form entries to annotate objects and collections. 

% Currently \dword{sam} values cover case a. and parameters cover case c. but it is not possible to list all possible values for a parameter without looping over all files. 

% 8.  Metadata can also contain “spill” or luminosity block information that allows a file to point to specific data taking periods with smaller granularity than a run or subrun. When files are merged, this spill information is also merged.


\subsubsection{Event Catalog}
\dword{sam} currently does not contain a means of easily determining which file a given event is in.  If a daq system is writing multiple streams, an event from a given subrun could be in any stream.   Existing neutrino experiments are low enough rate that this has not been an issue but protoDUNE already needs this feature.
The \dword{sam} replacement will require the development of a true a 2-way map between objects (events) and collections. 

 
% All of these features are intended to assure that your data are well described and can be found. As \dword{sam} stores full location information, this means any sam-visible location. In addition, if parentage information is provided, you can determine and reproduce the full provenance of any file.


\subsection{Datasets and projects}

\subsubsection{Datasets and Snapshots}

In addition to the files themselves, \dword{sam} allows you to define \dword{samdataset}s.

A \dword{sam} \dword{samdataset} is not a fixed list of files but a query against the \dword{sam} database. An example would be “data\_tier reconstructed and run\_number 2001 and version v10” which would be all files from run 2001 that are reconstructed data produced by version v10. This \dword{samdataset} is dynamic. If one finds a missing file from run 2001 and reconstructs it with v10, the \dword{samdataset} will grow. There are also \dword{samdataset} `\dword{samsnapshot}s` that are derived from \dword{samdataset}s and capture the exact files in the \dword{samdataset} when the \dword{samsnapshot} was made.
%Note: most other data catalogs assume a “\dword{dataset}” is a fixed list of files.  This is a “snapshot” in sam. 

%Do we need to keep the mutable query or stick to defined versioned collections? 


% Samweb – samweb is the command line and python API that allows queries of the \dword{sam} metadata, creation of \dword{dataset}s and tools to track and deliver information to batch jobs.

% Will need an updated API

\subsubsection{Projects}

%This is likely out of scope for the metadata themselves. 

Sam also supports access delivery and  tracking mechanisms called \dword{samproject}s and \dword{samconsumer}s.

A \dword{samproject} is effectively a processing campaign across a \dword{samdataset} which is owned by the \dword{sam} system. At launch a snapshot is generated and then the files in the snapshot are delivered to a set of \dword{samconsumer}s.  The \dword{samproject} maintains an internal record of the status of the files and \dword{samconsumer}s. Compute processes instantiate \dword{samconsumer}s attached to the \dword{samproject}.  Those \dword{samconsumer}s then request “files” from the \dword{samproject} and, when done processing, tell the \dword{samproject} of their status.  

The original \dword{sam} implementation actually delivered the files to local hard drives.  Modern \dword{sam} delivers the location information and expects the \dword{samconsumer} to find the optimal delivery method. This is a pull model, where the consuming process requests the next file rather than having the file assigned to it.  This makes the system more robust on distributed systems. 

There is also a web interface that allows users to view the status of running \dword{samproject}s. 

\href{http://samweb.fnal.gov:8480/station_monitor/dune/stations/dune/projects}{http://samweb.fnal.gov:8480/station\_monitor/dune/stations/dune/projects}



% Suggestions for configuring sam.

% First of all, it really is nice to have filenames and dataset name that tell you what’s in the box, although not required. The D0 and MINERvA conventions have been to use “_” underscores between useful key strings. As a result, D0 and MINERvA tried not to use “_” in metadata entries to allow cleaner parsing. “-“ is used if needed in the metadata.

% D0 also appended processing information to filenames as they moved through the system to assure that files run through different sequences had unique identifiers.

% Example: A Monte Carlo simulation file generated with version v3 and then reconstructed with v5 might look like

% SIM_MC_020000_0000_simv3.root would be a parent of RECO_MC_020000_0000_simv3_recov5.root
% Data files are all children of the raw data while simulation files sometimes have more
% complicated ancestry, with both unique generated events and overlay events from data as parents.

% Setting up \dword{sam} metadata

% This needs to be done once, and very carefully, early in the experiment. It can grow but thinking hard at the beginning saves a lot of pain later.

% You need to define data tiers. These represent the different types of data that you produced through your processing chain. Examples would be raw, pedsup, calibrated, reconstructed, thumbnail, mc-generated, mc-geant, mc-overlaid,

% run_type can be used to tell test beam from near detector from …

% data stream is often used for trigger subsamples that you may wish to split data into (for example pedestal vs data runs).

% Generally, you want to store data from a given data_tier with other data from that tier to facilitate fast sequential access. 

% Applications

% It is useful, but not required to also define applications which are triads of “appfamily”, “appname” and “version”. Those are used to figure out what changed X to Y. There are also places to store the machine the application ran on and the start and end time for the job.

% samweb list-files "data_tier raw and not isparentof: (data_tier reconstructed and appname reco and version 7)"

% Should, in principle, list raw data files not yet processed by version 7 of reco to produce files of tier reconstructed. You would use this to find lost files in your reconstruction after a power outage.

% It is good practice to also store the name of the head application configuration file for processing but this does not have a standard “value”

Example file metadata from DUNE. Note that many of the values are specific to the run configuration, not the individual file. The items with "." in them are parameters defined by the experiment while the rest are generic values intrinsic to \dword{sam}.

\begin{verbatim}

{
 "file_name": "np04_raw_run005141_0015_dl10_reco_12736632_0_20181028T182951.root", 
 "file_id": 7352771, 
 "create_date": "2018-10-29T14:59:42+00:00", 
 "user": "dunepro", 
 "update_date": "2018-11-28T17:07:30+00:00", 
 "update_user": "schellma", 
 "file_size": 14264091111, 
 "checksum": [
  "enstore:1390300706", 
  "adler32:e8bf4e23"
 ], 
 "content_status": "good", 
 "file_type": "detector", 
 "file_format": "artroot", 
 "data_tier": "full-reconstructed", 
 "application": {
  "family": "art", 
  "name": "reco", 
  "version": "v07_08_00_03"
 }, 
 "event_count": 108, 
 "first_event": 21391, 
 "last_event": 22802, 
 "start_time": "2018-10-28T17:34:58+00:00", 
 "end_time": "2018-10-29T14:55:42+00:00", 
 "data_stream": "physics", 
 "beam.momentum": 7.0, 
 "data_quality.online_good_run_list": 1, 
 "detector.hv_value": 180, 
 "DUNE_data.acCouple": 0, 
 "DUNE_data.calibpulsemode": 0, 
 "DUNE_data.DAQConfigName": "np04_WibsReal_Ssps_BeamTrig_00021", 
 "DUNE_data.detector_config": "cob2_rce01:.. 4 more lines of text", 
 "DUNE_data.febaselineHigh": 2, 
 "DUNE_data.fegain": 2, 
 "DUNE_data.feleak10x": 0, 
 "DUNE_data.feleakHigh": 1, 
 "DUNE_data.feshapingtime": 2, 
 "DUNE_data.inconsistent_hw_config": 0, 
 "DUNE_data.is_fake_data": 0, 
 "runs": [
  [
   5141, 
   1, 
   "protodune-sp"
  ]
 ], 
 "parents": [
  {
   "file_name": "np04_raw_run005141_0015_dl10.root", 
   "file_id": 6607417
  }
 ]
}



\end{verbatim}

% Merging and splitting

% Parentage works pretty well if one is merging files but splitting them can become problematic as it makes the parentage structure pretty complex.

% Sam will let you merge files with different attributes if you don’t check carefully. Generally, it is a good idea not to merge files from different data tiers and certainly not from different data_types. Merging across major processing versions should also be avoided.


% Example: Execute SAMweb Commands

% There is documentation at 
% https://cdcvs.fnal.gov/redmine/projects/sam/wiki/User_Guide_for_SAM

% https://cdcvs.fnal.gov/redmine/projects/sam-main/wiki/Updated_dimensions_syntax
% This exercise will start you accessing data files that have been defined to the DUNE Data Catalog. Execute the following commands after logging in to the DUNE interactive node, creating the directories above - 
% Once per session 
% setup sam_web_client #(or set up your standalone version)
% export SAM_EXPERIMENT=dune

% Then if curious about a file: 
% samweb locate-file np04_raw_run005141_0001_dl7.root
% this will give you output that looks like 
%  rucio:protodune-sp
%  enstore:/pnfs/dune/tape_backed/dunepro/protodune/np04/beam/detector/None/raw/06/60/59/05(596@vr0072m8)
%  castor:/neutplatform/protodune/rawdata/np04/detector/None/raw/06/60/59/05
%  cern-eos:/eos/experiment/neutplatform/protodune/rawdata/np04/detector/None/raw/06/60/59/05
% which is the locations of the file on disk and tape. We can use this to copy the file from tape to our local disk. 
% Better yet, you can use xrootd to access the file without copying it if it is staged to disk. 



% Find the xrootd uri via 
%  samweb get-file-access-url np04_raw_run005141_0001_dl7.root --schema=root

% root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/tape_backed/dunepro/protodune/np04/beam/detector/None/raw/06/60/59/05 /np04_raw_run005141_0001_dl7.root
% root://castorpublic.cern.ch//castor/cern.ch/neutplatform/protodune/rawdata/np04/detector/None/raw/06/60/59/05/np04_raw_run005141_0001_dl7.root
% root://eospublic.cern.ch//eos/experiment/neutplatform/protodune/rawdata/np04/detector/None/raw/06/60/59/05/np04_raw_run005141_0001_dl7.root

% You can localize your file with the --location argument (enstore, castor, cern-eos) 
%  samweb get-file-access-url np04_raw_run005141_0001_dl7.root --schema=root --location=enstore

% root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/tape_backed/dunepro/protodune/np04/beam/detector/None/raw/06/60/59/05 /np04_raw_run005141_0001_dl7.root

%  samweb get-file-access-url np04_raw_run005141_0001_dl7.root --schema=root --location=cern-eos

% root://eospublic.cern.ch//eos/experiment/neutplatform/protodune/rawdata/np04/detector/None/raw/06/60/59/05/np04_raw_run005141_0001_dl7.root

% Or you can use the location you got with the locate-file command to do a direct read (be sure to strip the enstore: or eos: at the beginning). 
% /pnfs/dune/tape_backed/dunepro/protodune/np04/beam/detector/None/raw/06/60/59/05/np04_raw_run005141_0001_dl7.root

% To get \dword{sam} metadata for a file for which you know the name: 
% samweb  get-metadata np04_raw_run005141_0001_dl7.root # add the --json option to get output in json format
% To list raw data files for a given run: 
% samweb list-files "run_number 5141 and run_type protodune-sp and data_tier raw"
% What about a reconstructed version? 
%  samweb list-files "run_number 5141 and run_type protodune-sp and data_tier full-reconstructed and version (v07_08_00_03,v07_08_00_04)"

% gives 

%  np04_raw_run005141_0001_dl7_reco_12736115_0_20181028T165152.root

% then 

% samweb get-file-access-url np04_raw_run005141_0001_dl7_reco_12736115_0_20181028T165152.root --schema=root

% root://fndca1.fnal.gov:1094/pnfs/fnal.gov/usr/dune/tape_backed/dunepro/protodune/np04/beam/output/detector/full-reconstructed/07/34/21/02/np04_raw_run005141_0001_dl7_reco_12736115_0_20181028T165152.root
% root://eospublic.cern.ch//eos/experiment/neutplatform/protodune/rawdata/np04/output/detector/full-reconstructed/07/34/21/02/np04_raw_run005141_0001_dl7_reco_12736115_0_20181028T165152.root

% samweb allows you to select on a lot of parameters 
% Useful ProtoDUNE samweb parameters 
% https://dune-data.fnal.gov lists some official dataset definitions 
% You can make your own samweb dataset definitions
% First check to see what you will get 
% samweb list-files "data_tier full-reconstructed and version v07_08_00_04  and data_stream cosmics and run_type protodune-sp and detector.hv_value  180" –summary

% samweb create-definition $USER-v07_08_00_04_good_cosmics "data_tier full-reconstructed and version v07_08_00_04  and data_stream cosmics and run_type protodune-sp and detector.hv_value  180"
% Note that the username appears in the definition name - to prevent users from getting confused with official samples, your user name is required in the definition name. 
% prestaging
% At CERN files are either on eos or castor. At FNAL they can be on tape_backed dcache which may mean they are on tape. 
% setup fife_utils -t # a new version we requested 
% sam_validate_dataset  --locality  --file np04_raw_run005141_0015_dl10.root --location=/pnfs/
% returns ONLINE_AND_NEARLINE: 1 if available on disk 
% sam_validate_dataset  --locality  --file np04_raw_run004590_0001_dl1_decoder_11786241_0_20180924T033049.root --location=/pnfs/
% locality counts:
% NEARLINE: 1
% No ONLINE means you need to prestage that file. Unfortunately, prestaging requires a definition 
% Let's find some for run 5141 
% samweb list-definitions | grep 5141
% runset-5141-raw-180kV-7GeV-v0
% runset-5141-reco-v07_08_00_04-hv-180kV-beam-7GeV-v0
% runset-5141-raw-v0
% runset-5141-reco-v07_08_00_04-v0
% runset-5141-reco-v07_08_00_03-v0
% protodune-sp_runset_5141_reco_v07_08_00_03_v0
% protodune-sp_runset_5141_raw_v0
% protodune-sp_runset_5141_reco_v07_08_00_04_v0
% protodune-sp_testset_5141_raw_v0
% protodune-sp_testset_5141_reco_v07_08_00_04_v0
% protodune-sp_testset_5141_reco_v07_08_00_04_v1
% protodune-sp_testset_5141_raw_v1
% hyliao_run_5141
% runset-5141-reco-unified-hv-180kV-beam-7GeV-v0
% hyliao_sn_run_5141
% samweb prestage-dataset --def=runset-5141-reco-unified-hv-180kV-beam-7GeV-v0 --parallel=10
% would prestage all of the reconstructed data for run 5141 
% you can check on the status by going to 
% http://samweb.fnal.gov:8480/station_monitor/dune/stations/dune/projects and scrolling down to see your prestage link. 
%\end{document}