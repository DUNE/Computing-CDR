\documentclass[../main-v1.tex]{subfiles}
\begin{document}

\chapter{Frameworks}\label{ch:fworks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{xyz}
%\label{sec:fworks:xyz}  %% fix label according to section

DUNE presents a unique challenge for data analysis and data processing. For DUNE to be successful, we will need to be able to extract high fidelity physics information from the reconstruction, processing and filtering of large datasets, which are organized as variable time-series.   DUNE has an ambitious physics program that spans numerous physics topics including precision neutrino oscillation measurements, searches for proton decay, sensitivity to nearby SuperNova explosions, and more. But these physics interactions occur at vastly different timescales from nanoseconds to 100s of seconds. A framework that is fully capable of adapting to these varied timescales efficiently will be important. And accomplishing timely results will require that data from the DUNE far and near detectors  be efficiently processed using modern computing techniques and that traditional \dword{hep} computing techniques be adapted and coded to run on new generations of advanced and accelerated computing resources such as \dword{hpc}, \dwords{gpu} and other novel computing architectures.  

Development of this type of modern computing pipeline and analysis structure will require the development and reoptimize the formal computing frameworks that are used for data manipulation by the DUNE experiment and to large extent the greater neutirno community to which DUNE and its computing ecosystem belong.

In this Chapter we provide a brief overview of our existing code bases and methodologies followed by a description of our process for designing the optimal frameworks for data processing and analysis that will be needed when the full \dword{dune} begins operations.  

\section{Defining a Framework}\label{sec:framework:def}

Modern high energy physics experiments generate large volumes of primary detector data, as well as auxiliary and ancillary data from secondary systems, and vast amounts of simulation which mimic these detectors and their responses to provided estimates and predictions for the different physics processes that may be observed in the experiments.  The management, flow, formal structure, transformations, analysis and interaction of these data and their algorithms require a well defined framework for governing and sequencing these tasks.  In particular in \dword{hep} analyses, data processing and analysis frameworks are designed to provide "event loops" which allow for experimenters to step through the data and simulation in a manner that can break up the information into discrete chunks which represent physical processes.  These discrete chunks are referred to as the "data processing atoms".

Historically in \dword{hep} experiments the data atoms have been associated with well defined external time structures which govern the data collection.  For collider experiments these are often associated with a beam or bunch crossing, while for fixed target experiments they have often been associated with pulsed beam extraction to target stations (often referred to as a "spill"), or substructure within the beam spills defined by the data acquisition and triggering systems (i.e. the data atoms are the individual triggers).  

In the case of DUNE, the DUNE readout systems, and the DUNE physics mission the definition of these data atoms is not as simple or as static as it has been historically in other experiments.  While neutrino beam interactions have natural timescales of nanoseconds, the timescale of supernova neutrino bursts, proton decay searches, and long-lived, BSM particle searches occur at timescales up to 100s of seconds. The nature of these varying times scales within the DUNE physics program, results in data atoms which vary in size and structure, and may be spread across numerous files and data structures. These data atoms could represent a) an arbitrary time window representing a time period of interest b) a time window representing the drift of ionization across the detector volume c) a spatially and temporally isolated region of the detector readout d) a sub-region of interest within a spatially/temporally distinct portion of the detector or e) other segmentations of the DUNE detector readouts corresponding to potential physical signatures with time structures defined by the physical process. Having a framework that has adaptability to varying time structures, multiple input files, and data structures is an important feature for DUNE to accomplish its vast physics goals.

In this manner, the DUNE data framework is a formal software structure and engine that can ingest and apply transformations and filters to the individual data atoms regardless of their underlying nature and related time structure.  The framework has a responsibility for controlling and managing the I/O associated with ingesting the data atoms, for sequencing and scheduling the algorithms and transforms that run on the data, and for providing logging, provenance and other bookkeeping tasks associated with describing and enumerating how the data was handled and new information derived.  

To these ends, the framework provides mechanisms (such as "plug-in" structures) for experimenters to develop modular algorithms which can be included or excluded from a given analysis chain and be run either serially or in parallel depending on the data context.  The framework also provides mechanisms for data to be passed between different algorithmic portions of the data processing and analysis chain while maintaining the data's coherence and integrity from the standpoint of the computing platform.  In some framework implementations this data passing mechanism is provided by a {\it data store} which includes controlled forms of data access and locking.  Data stores can be used for both transient and long term information passing.  Frameworks which use these types of methods for data passing need to then additionally manage memory footprints of the stores to ensure that they can operate within the technological memory capacities provided by the platforms on which the codes run.  Other frameworks can provide less ridge data pipelines for information passing which can be advantageous when dealing with large data streams and which can better accomodate ingress and egress data paths with respect to the bulk memory footprints, but which may not be compatible with certain types of parallel processing and parallel algorithms, or with algorithm scheduling systems.

The other key feature that modern frameworks provide, are the ability to configure and re-configure the content and flow of the execution modules without the need to rewrite the underlying analysis routines.  This aspect of a framework as a configuration driven entity is key to their flexibility and their ability to satisfy the needs of the many different and varied analyses that are proposed for DUNE.  Without a robust configurable framework, different data processing tasks and analysis chains could be spread across a wide variety of codes and computing solutions, which could severely hamper the collaboration's ability to understand the derivation of results and ensure their integrity during scientific reviews.  A robust configuration driven framework system allow for both minor changes to individual parameters, or perterbations on those parameters to investigate the data or simulation's response to those changes, and allows for reordering, restructuring and inclusion/removal of algorithmic blocks without the need to edit or recompile the core physics codes.  This in turn promotes code stability, reuse, and facilitates modern code design and debugging principles.  

The other key concept that defines modern data processing frameworks, are their construction and organization as advanced state machines.  This allows for the execution of more complicated workflow toplogies, parallel analysis chain topologies, and dynamic execution models, than older linear execution techniques allowed for.  This state machine organization and behavior also allows the frameworks to emulate better the behavior of other systems that are used throughout  \dword{hep}, such as those encountered in the data acquisition systems realm, and to adapt more readily to parallel and asynchronous data access systems which the coherence of the system's ``state'' can be monitored and maintained between framework controlled state transitions (i.e. between configuration, initialization, algorithm execution, data serialization, and finalization stages).  

It should be noted that in modern framework definitions and designs, there is typically an interplay and hand off between the framework which is executing the algorithm code, and the higher level workflow management layers which are responsible for the batch scheduling and delivery of the framework ``jobs'' to compute resources at different computing sites or on different types of computing hardware.  Modern frameworks are typically designed to be aware of this macro level scheduling layer, and will often provide hooks to allow for the management layers to either transmit or receive information from the framework jobs regarding their configuration and various types of diagnostics which can help both the framework and the workflow management layers more efficiently execute their missions (e.g. a framework will often provide monitoring diagnostics regarding the overall progress of the job, which then allows for the management layer to stage queue work or initiate data transfers between sites,  while the workflow system may provide ``late binding'' configuration information to the framework which allows it to determine at runtime sources of input data, or locations for output data which may be site specific or coordinated by the higher level management tools).

Lastly, modern frameworks as used by dword{hep}\ today are also designed and responsible for adapting to  heterogeneous computing resources.  Modern frameworks must, due to the push towards exascale computing platforms, be able to execute their codes or bind/link their codes across not only different operating systems, but across different hardware architectures.   In the case of DUNE this will be an enabling technology that allows for the DUNE algorithms to run on platforms ranging from commodity desktop/laptop systems, to large scale grid computing centers, and across the exascale era leadership computing facilities being built by the Department of Energy and other major national and international computing centers and spaning architectures from the classic x86 CPU architectures to advanced many core GPU and AI/ML tuned accelerator systems.  The framework's ability to deal with this diversity will be required by DUNE and is discussed in detail in the following sections.

\section{Current status}\label{sec:framework:status}

Currently a set of frameworks have been used for the simulation of the DUNE detectors, and processing of data from the ProtoDUNE detectors.  These frameworks have been developed or adopted based on the immediate needs of the sensitivity studies, simulation studies and ProtoDUNE running.  These frameworks are based off of earlier frameworks that have been used in the neutrino community and reflect in part the features that are needed for long and short baseline neutrino analysis.

For the \dword{fd} simulation and reconstruction efforts, the \dword{protodune} detectors and \dword{ndgar} studies, the \dword{art} framework which was developed at Fermilab is the primary framework being used throughout the collaboration.  The \dword{art} framework, was originally developed based off of the CMSSW framework used by the CMS experiment, but was designed to meet the specific needs of the neutrino and muon science communities.  The \dword{art} framework was designed as a multi-experiment framework starting in 2011~\cite{Green:2012gv}, and is currently used by 11 different major experiments in the HEP community including DUNE, NOvA, MicroBooNE, ICARUS, SBND, Muon g-2 and Mu2e.  It is developed, maintained and supported by Fermilab's \dword{scd} and through a stakeholders committee consisting of the experiments using the framework.

The framework provides the data processing loop, manages memory, interfaces to I/O tools, defines uniform mechanisms for defining, associating, and persisting data products, provides a uniform mechanism for job configuration, stores job configuration information in its output files, and manages messages, random numbers and exceptions.  The framework is highly modular and common modules which are shared across experiments have been developed for common infrastructure components such as the accelerator beam information systems at Fermilab, and various data management systems and data cataloging systems, and database systems for calibration and conditions data access.  Use of the \dword{art} framework has allowed for DUNE collaborators and other experimenters to quickly interface with Fermilab facilities, and with other facilities hosted by CERN and other sites, in a common and consistent manner.  The framework also allows for dynamic (runtime) configuration of the code modules, which has allowed for additional common code libraries to be developed and run on top of the \dword{art} base. 

In particular, the \dword{larsoft} toolkit is a collection of \dword{art} plug-ins and associated algorithm code, configuration files, static data such as geometry specification files and photon visibility maps which has been developed for the liquid argon TPC detector community.  \dword{larsoft} provides the interface to neutrino event generators such as \dword{genie} and cosmic ray generators and simulations such as \dword{corsika} and \dword{CRY}, detector simulation via \dword{geant4}, custom simulation and reconstruction software, event displays and tutorials.  Experiment-specific metadata and configuration database plug-ins assist in batch workflow organization.  Like \dword{art}, \dword{larsoft} is supported by Fermilab's \dword{scd}, and through collaboration and contributions from participating experiments.

For the gaseous detector simulations, the \dword{garsoft} software package is used.  \dword{garsoft} is patterned on \dword{larsoft} as a layer of common algorithms which runs on top of the \dword{art} framework.  It provides a robust toolkit for simulating and reconstructing data from the \dword{ndgar} concept detector or for other gaseous argon detectors.  Like \dword{larsoft}, it provides interfaces to event generators and \dword{geant4}, custom simulation and reconstruction software, and event displays.  It also provides simulation and reconstruction for \dword{ndgarlite}.  Unlike \dword{larsoft}, \dword{garsoft} is written, maintained, and supported only by the DUNE collaboration and is not shared with other experiments in the neutrino or HEP community.

In addition to the \dword{ndgar} software, the DUNE collaboration's near detector groups have developed different tool suites for simulation and analysis of the proposed near detector designs.  The \dword{ndlar} software, which represent a group of standalone tools for simulating and reconstructing pixel-based \dword{lartpc} data.  These tools and toolchains have been developed for simulating and analyzing \dword{singlecube} prototype data.  The \dword{sand} software efforts are based off of collaborator's experience with the \dword{kloe} detector and its software.  The portions of the \dword{sand} software are being used to provide simulation the magnet and calorimeter systems, while new software is currently being developed for the \dword{3dst} and other components of SAND.  These software stacks are specifically being developed with maximum flexibility due to the design stage of the SAND concept.  This software flexibility is important at this stage in the design in order to allow studies of different detector designs which can be used between the \dword{3dst} and the calorimeter.  These tools are currently only used by the near detector group and have an independent software structure from the long baseline groups.

For higher level analysis tasks, sensitivity studies, and event selection, DUNE is currently leveraging the \dword{CAFAna} framework.  \dword{CAFAna} is a high level framework based on the \dword{ROOT} analysis software stack, which is designed to work with \dword{ROOT} TTree's (ntuples).  \dword{CAFAna} is designed to provide bookkeeping facilities for neutrino flux information and exposures, which are needed for long baseline oscillation measurements and for short baseline cross section measurements.  \dword{CAFAna} is designed to work with output from data that has been processed with \dword{art} and \dword{larsoft} and provides a more interactive environment for experimenters to explore the data, by using the ROOT scripting and interpreter interfaces, with specific \dword{CAFAna} libraries and functions.  \dword{CAFAna} provides facilities for high level event selection, normalization of distributions, re-weighting distributions,  

The distinguishing features of two frameworks (\dword{art} and \dword{CAFAna}) are that \dword{art} works on the individual event record by event record level, while the \dword{CAFAna} framework allows for high level event selection, but operates primarily on the resulting ensemble level distributions.  

\section{Framework Requirements \hideme{Laycock/Norman - needs update}}
\subsection{Requirements Process} %-\hideme{Norman/Laycock in progress}}
% Taken from vCHEP paper
Modern \dword{hep} software frameworks are in the process of addressing the increasing heterogeneity of computing architectures that are redefining the computing landscape, work that is closely followed by the \dword{hsf}~\cite{Alves:2017she, Calafiura:2018rwe}. The \dword{dune} collaboration is keen to participate in these efforts to minimize unnecessary development effort in improving our own framework. 

Given the unique challenges that \dword{dune} data pose, in 2020, the collaboration assembled a task force to make a preliminary definition of the requirements for its software framework based largely on physics use cases \cite{bib:docdb21934}.  The collaboration then approached the \dword{hsf} who assembled a panel of software framework experts from various experimental backgrounds to review these preliminary requirements. The report \cite{bib:docdb24423} produced by this panel was discussed in a workshop involving the panelists and task force members, which resulted in a followup workshop to address the outstanding issues, mostly around the topic of concurrency.  The findings of the panelists, and the summary of the concurrency workshop \cite{bib:docdb24426}, have been incorporated into the \dword{dune} software framework requirements presented in the following.

\subsection{Summary of Use Cases}
\hideme{HMS 3/13 I added a table - probably need a row for a single ND event}
Table \ref{tab:fworks:tasks} summarizes the dominant use cases for the large scale reconstruction and simulation framework.

\begin{itemize}
\item Simulation for the \dword{fd} will likely span only a subset of a detector module as individual interactions are confined to a reasonably small volume. However, without substantial effort, all of the resulting simulated hits will need to remain in memory at once. Simulations of a 2x6 \dword{apa} region in the \dword{fdhd} are already standard and testing for \dword{fdvd} is ongoing. This is one place where the photon detectors may add substantially to the memory footprint as discussed in section \ref{ch:use:pd}. 

\item Reconstruction is already running into memory limitations when processing the 6 \dwords{apa} in one trigger record from \dword{pdsp} in memory at once.
 Most \dword{fd} Trigger Records, initiated by cosmic rays, beam neutrino interactions and atmospheric neutrino interactions, will have only one interaction in a Far Detector module, although  some cosmic rays arrive in bundles with other cosmic rays, even at the 4850’ level.
Processing a full \dword{fd} module, with hundreds of thousands of channels and thousands of samples/channel will require quite sophisticated memory management, with individual readout subunits such as \dwords{apa} or \dwords{crp} processed individually, either sequentially or in parallel.  The current \dword{art} framework is not optimally set up for this scenario. 



\item A SNB Trigger Record will be 100's of TB in size and may have thousands of interesting physics interactions in it.  They will be small tracks (“stubs”), of order of tens of wires hit in each plane, distributed through the detector and in time during the long Trigger Record.  A convenient analysis workflow will save regions of interest to smaller files containing only data needed to analyze the small tracks and not the large amounts of waveform data containing only electronics noise and radiologicals. 


\item The near detector, on the other hand, will have many neutrino interactions per LBNF spill.  The Gaseous Argon Near Detector Component (ND-GAr) expects 60 overlaid interactions per spill, mostly originating in its calorimeter, which has fast timing capabilities and thus can be used to separate particles belonging to different Trigger Windows.  The liquid-argon TPC near detector will have order of 20 interactions per spill.  Some downstream analyses will benefit from and expect upstream analyses to divide a Trigger Record’s data into subsets based on classification algorithms that are intended to separate one interaction from another.  These physics regions of interest, usually called “slices”, are then what physicists expect to use as a unit of execution, i.e. they loop over slices in their analysis code.
\end{itemize}

%The use cases for \dword{dune} are summarized in table \ref{tab:fworks:tasks}

\begin{dunetable}
[Summary of computing resources needed per trigger record]
{l r r r r r r }
{tab:fworks:tasks}
{Summary of resources needed per trigger readout for compute intensive tasks in \dword{protodune} and the \dword{fd}. These numbers assume that reconstruction will load individual subunits such as \dwords{apa} into memory one at a time, possibly in parallel across multiple cores. }
Use case	&memory &	input size &	output  size 	&	CPU time 	&	input/core  	& cores/readout		\\
units	&GB	& MB	&MB	&	sec	& MB/s	&		\\

Simulation+reco	&		6	&	100	&	2000	&	2700	&	0.00	&	1 to 150	\\
FD reco	&	4	&	80	&	4000	&	600	&	0.13	&	up to 150		\\
SNB reco	&	2	&	3.2$\times10^8$	&	?	&3-10$\times10^8$		&	0.13	&	10-30,000		\\
%tuple creation	&	3	&	4000	&	1	&	100	&	40.00	& 1		\\
%calibration	&	3	&	4000	&	1	&	100	&	40.00	& 1		\\
%Parameter estimation	&		&		&		&		&		&			\\
\end{dunetable}

%Step | Contexts | Memory requirement for one data atom (MB) |

%THIS IS TABLE BLAH
%Would be useful to identify "production" use cases and "analysis" use cases.


One striking feature of the use cases that \dword{dune} needs to support is the diversity in scale of the data processing atom.  Somewhat counter-intuitively, although the raw trigger records are themselves very large, the data processing atom (a single \dword{apa} or \dword{crp}) is relatively small and compatible with the same high-throughput computing workflows adopted by most \dword{hep} experiments.  Meanwhile at the analysis level although the quantity of data related to a single trigger record is small, nuisance parameter extraction requires correlations across all trigger records in an analysis dataset, which is the effective data processing atom.  This is a very good fit to high performance computing, particularly if the analysis framework can take advantage of many \dword{hpc} nodes at the same time. Experience from the \dword{hsf} panelists encouraged \dword{dune} to separate the analysis and production use cases when considering the software framework design.


%\subsection{Data and I/O layer}

%As noted the discussion on raw data processing, the DUNE software framework must have the {\bf ability to  change its unit of data processing} from single planes to supernova readouts with extreme flexibility and ensure that memory is treated as a precious commodity.  

%% This is really talking about slices, covered in Analysis
%It must be possible to correlate these units to DUNE data-taking "events" for exposure accounting, and experiment conditions.


%{\bf Partial reading of data} is another strong requirement needed to keep memory usage under control.  There must be {\bf no assumptions on the data model}, {\bf the framework must separate data and algorithms} and,  further, {\bf separate the persistent data representation from the in-memory representation} seen by those algorithms.  DUNE data is well suited to massively parallel processing compute architectures, so the framework will need to {\bf have a flexible I/O layer supporting multiple persistent data formats} as different formats may be preferred at different stages of data processing.  The sparsity of DUNE data also imply that compression will play an important role in controlling the storage footprint, especially for raw data, and the sparsity of physics signals further emphasize support for data reduction (skimming, slimming and thinning) in the framework.  The framework {\bf needs to be able to handle the reading and writing of parallel data streams}, and navigate the association (if any) between these streams with a {\bf critical need to allow experiment code to  mix simulation and (overlay) data}.

%37 - this could be tied to req #1, separation of data and algorithms?
%--REQ ON DUNE, NOT THE FWK - Calibration, reconstruction, and selection algorithms must be framework-agnostic, i.e. able to run transparently in any official DUNE framework where equivalent requisite data products exist.

%%% Andrew's notes on memory management
\subsection{Memory Footprints in DUNE}
The DUNE \dword{lartpc} data is naturally organized into geometrically distinct regions corresponding to \dword{apa} or \dword{crp} regions .  In the far detector, the uncompressed readout is estimated at 6 GB/readout, but can be subdivided into 150-160 separate 40 MB/readout regions corresponding to each \dword{apa}/\dword{crp}.  The actual data footprint that a DUNE data processing algorithm requires to be present in core memory varies depending on the total number of \dword{apa}/\dword{crp}s on which the algorithm needs to operate on at a given time.  (i.e. if an algorithm needs the entire readout it requires a minimum of 6 GB of memory, while if it operates on only a single \dword{apa}/\dword{crp} then it requires a minimum of only 40 MB.)   Many DUNE algorithms are transformations of the base data and require a multiple of the base memory footprint to be computed (i.e. large matrix style multiplications require the base data, transformation matrix and resultant matrix.)  Other operations require data from temporally adjacent readout windows and similarly require a multiple of the base data size to be accomplished.

\subsection{I/O Requirements for the Simulation/Reconstruction Framework}

For the purpose of these requirements we have considered the known limiting cases of data processing as being the drivers for memory requirements.  While optimization of specific algorithms may be possible, we consider the generalized unoptimized cases at this time are representative of real world data processing.
%%%%

Given the uncertainty in the choice of data format:

\begin{itemize}
\item  the framework must support reading and writing different persistent data formats

%2
\item the framework must separate the persistent data representation from the in-memory representation seen by algorithms

%1
\item the framework must separate data and algorithms 

For developers, highly modular code must be encouraged, allowing evolution or replacement of sub-algorithms that lend themselves to particular approaches.  It is assumed that algorithmic code will be organized in “algorithm modules” and this term is used in this document.  The codebase will therefore likely contain several alternatives for (sub)algorithms, the choice of which would depend on the available hardware.  The framework will need to run in heterogeneous and potentially dynamic environments where the availability and type of co-processors may be known late.  While the design of this technically challenging aspect is left to framework developers, it is worth pointing out the added challenge of developing algorithms for diverse hardware if the framework does not easily allow it.  Therefore:

%11 - related to separating data from algos or belongs there?  Not concurrency
\item it is highly desirable that it be possible to write algorithmic code independently of the framework.

This also helps keep the gap between analysis and production code as low as possible.  Furthermore, 

%12
\item it is highly desirable that the framework be sufficiently modular in design to allow re-use of framework services and functionality both within and outside of the framework context, as far as that is possible.



%The first stage of processing in offline jobs, after job configuration and initialization, is reading in detector data.  Offline jobs must read in data produced directly by the data acquisition system and also data produced by Monte Carlo simulation.  Input data files may be retrieved from a persistent storage system, delivered over a network, or reside on local storage.

%% Andrew added a comment on needing to specifically support both row-wise and column-wise representations of data but this seems to be getting into the I/O implementation?  Could be used as an example rather than a requirement.

Every version of the framework must be able to read data files written by that version of the framework and all previous versions, with no loss in functionality or change in meaning of the data elements:

%22
\item The framework I/O read functionality must be backward compatible across versions.

We do not require forward compatibility, in which data written with newer versions of the framework are also readable by older versions.  Cases in which forward compatibility is broken need to be documented as far as possible, however, as these are breaking changes.  

Experimenters may change their minds about the contents of data products.  For example, data members may be added because they were initially not included in the design but later found to be necessary, and rather than create a new data product, an expansion of an old one is more convenient.  Framework programs reading old and new data files need to behave seamlessly if the data product has changed definition.

%23
\item The framework I/O layer needs to provide a mechanism for user-defined schema evolution of data products.

The DUNE data model allows for Trigger Record data to be stored in persistable representations which are generated by customized hardware or which are optimized for specific acceleration hardware or computing systems.  As a result, the data model expects that data will have custom “packed” representations that do not conform to 32-bit or 64-bit little-endian words. Furthermore, compression of the raw waveform data will be performed in the DAQ, though some data may arrive uncompressed.  Some highly compressible data products may benefit from dedicated compression algorithms scheduled to run before output. Therefore, 

%24
\item the framework I/O layer must provide a mechanism to register/associate and to run/apply custom serialization/deserialization and compression/decompression algorithms to data on write/read of that data from a persistable form.  

Data products that do not have dedicated compression algorithms associated with them can still benefit from automatic compression that is enabled by default.

%25
\item The framework I/O layer should support compression on output data in a manner that is transparent to users and is configurable.  It must be possible to disable the automatic compression of output data.

Experience shows that it is highly desirable to be able to configure a maximum file size such that output files are the correct size for efficient storage and for units of data processing; currently a file size of several GBs is considered optimal.  As this requires the closure of an existing output file and creation and opening of a new file (with sensible filename) then this needs to be addressed at the framework level.

%26
\item The framework I/O layer should allow a configurable maximum output file size and provide appropriate file-handling functionality.
% What does "appropriate" mean?




An issue that arises during data read-in and decompression and unpacking is the memory footprint used.  The DUNE Far Detector is big.  Supernova-burst (SNB) processing in the DUNE Far Detector presents unique challenges due to the large volume of data that are produced in each Trigger Record.  An uncompressed SNB Readout for 100 seconds will take about 120TB of storage for one single-phase far detector module for just the TPC wire data, and DUNE will have four detector modules.  These data will be divided into smaller chunks both in time and by detector component.  For single-phase detector modules, these components are likely to be the anode plane assemblies (\dword{apa}/\dword{crp}) due to the granularity of the data preparation processing.

In the first stage of offline processing, waveforms from the channels are de-noised and deconvolved, and pulses that are approximately Gaussian appear in the processed waveforms. Because the 100s of SNB Readout from each channel must be artificially broken into small chunks in time, there is the potential to introduce edge effects at the chunk boundaries. In order to avoid this, about 100 microseconds of data spanning the boundary on both sides must be used in processing the chunks.

A common ratio of RAM to CPU cores on existing grids is 2 GB/core.  Memory usage beyond this results in poor performance and can lead to job eviction depending on the resource configuration.  An uncompressed DUNE Far Detector module Trigger Record will be larger than this, about 6 GB.  A supernova Trigger Record of 100s will be more than five orders of magnitude bigger again.  Clearly the framework will need to be able to act on subsets of Trigger Records, while respecting the overlap criteria noted above in order to avoid creating artefacts.

%27
-- CRITICAL REQ The framework must be able to operate on subsets of a Trigger Record.  Specifically, it must be possible to break Trigger Records down into smaller chunks (e.g. one \dword{apa}/\dword{crp}) and be able to stitch those chunks back together.  For supernovae, it must also be possible to reuse a fraction (nominally 100 us) of the previous chunk (nominally 5ms) of data to allow stitching in time.

Data unpacking and initial processing can be arranged to operate on these subsets.  In order to realize the benefit from operating in this way, however, intermediate data products that are no longer needed must no longer occupy space in memory.  Data products that have been written out and are no longer needed in memory are also good examples of those that can be evicted from memory, but there are also cases of intermediate products which must be flushed instead of written out.

%28 NEED TO DEFINE "USEFUL", and what does this really mean as a requirement?
\item Data products should not occupy memory beyond their useful lifetimes.

As noted in “General Considerations” (!!!!!!!), the framework must be aware of which algorithms need which data products, while the algorithmic code, by nature of its modularity and re-use/reconfigurability, is expected to be unaware of what other components may be run in the same job.  Therefore, the framework must be responsible for garbage collection, capable of freeing up memory at the earliest possible time.  Furthermore, the framework will need to respect the memory constraints imposed by the processing environment which will entail supporting partial reading of data objects into memory and potentially purging any data objects or partial data objects not immediately required.

%29
\item The framework must manage memory of data products in the Data Store.

Frameworks need to provide configurable, flexible I/O access so that experiments can control the output of their jobs in fine-grained detail.  This is needed to save on storage and also experimenter time, as smaller datasets take less time to analyze than larger ones.
 
%30
\item The framework needs to support skimming/slimming/thinning for data reduction.

Similarly, processing and analysis is made much more convenient (or even possible) if the skimmed/slimmed/thinned output streams can be associated with information in other streams that may be stored separately.  Some analyzers may need the auxiliary data streams while others may not, and so a framework job that produces outputs for collaboration use would need to read all of the necessary streams.  This also allows efficient use of storage, as data does not need to be co-located in the same file to be available for processing.  In the case of writing, I/O cost can be very efficiently amortized if several output streams can be written based on one input file.

%31
\item The framework needs to be able to read and write several parallel data streams.  Labeling of data objects across streams should be intuitive and not error prone.  Provenance information should support correlating related data objects across streams.

A common offline job need that goes beyond the 1->1 input to output data model is mixing real Trigger Record data with Monte Carlo simulation.  Monte Carlo simulation is often not sufficiently realistic, perhaps it fails to capture the time dependence of detector conditions or the generator or detector simulation simply lacks sufficient accuracy for the physics use case.  In this case, Monte Carlo simulation can be augmented by adding actual detector data, e.g. to embed single tracks or entire Trigger Records into simulated data and reconstruct them as if they were actual Trigger Records.  This can lead to a 2->many input output situation with asynchrony in both input and output. 

%32
\item The framework needs to allow experiment code to mix simulation and (overlay) data.


%36
\item The framework must support partial reading of the persistent data and must not require reading an entire Trigger Record unless required (i.e. it must not force the entire Trigger Record to be read).
%---

Efficient data access for typical analysis workflows is also very different to data processing using the event-by-event paradigm. It is very common to require access to only a small subset of the Trigger Record information (the variables required to make a cut, or reconstruct a quantity), and for the amount of information to vary across Trigger Records (e.g. in the case of early rejection in a physics analysis selection).

\end{itemize}




\subsection{General Requirements} % Fancier name ?

The reproducibility of physics results and the knowledge of how physics results were obtained is essential to DUNE and to the neutrino community as a whole.  It must be possible both to replicate physics results using identical input data, or to repeat an analysis using a different set of input data with an identical sequence of identically configured algorithms. 

\begin{itemize}
\item The framework must ensure that memory is treated as a precious commodity, implying that {\bf intermediate data products cannot occupy memory beyond their useful lifetime}.  Nevertheless, reproducibility is a key requirement of any scientific tool and {\bf the framework must provide a full provenance chain} for any and all data products which must include enough information to reproduce identically every persistent data product. By definition, the chain will also need to include sufficient information to reproduce the transient data passed between algorithm modules, even though the product is not persisted in the final output.

 For the purpose of these requirements, we consider data to be reproducible, if:
The products in a given event record are bitwise identical for integer fields
Floating point numbers in a given event record are the same to within the floating point precision of the machine they were generated on.
The ordering of data products within an event record are the same.
These criteria for reproducibility are designed to allow DUNE simulate “rare” events with well defined single event sensitivities, and to perform numeric transform operations on data values across different hardware platforms and runtime concurrency topologies.

\item It is also highly desirable that the framework broker access to random number generators and seeds in order to guarantee reproducibility.  Random number generation is an important aspect of code and given the additional complications of multi-threading and co-processors:

%16
\item it is highly desirable that the framework broker access to random number generators and seeds in order to guarantee reproducibility.  

All of the preceding considerations imply that {\bf the software framework will need a very robust configuration system} capable of handling the requirements in a consistent, coherent, and systematically reproducible way.

%14
\item the framework must provide a full provenance chain for any and all data products which must include enough information to reproduce identically every persistent data product.  By definition, the chain will also need to include sufficient information to reproduce the transient data passed between algorithm modules, even though the product is not persisted in the final output.  
\end{itemize}

\subsubsection{Configuration}

Configuration is distinct from initialization of the framework objects; configuration happens first.  Given that RAII is an important concept for multi-threaded design, it is best to have a fully configured framework instance in the initialization phase.  Given the abundance of variations that make up \dword{hep} workflows a robust and easily programmable configuration system is a foundational component of all modern frameworks.  Some use a strictly declarative language and some use a Turing complete language.  The former must be augmented with scripts that write the declarations in a Turing complete language (usually it is part of the workflow management system, WMS) because it turns out that control flow is a requirement.  Given this, there is a requirement that:

\begin{itemize}
%3 - HSF really didn't like this requirement
\item NEEDS UPDATING TO REMOVE TURING - the framework should provide a Turing complete configuration language as a foundational component so that it can ensure coherence of its configuration.

\todo{May be better to move/duplicate the mention of input file(s) into the I/O section. Workflow Allocator will supply the next file to process on request, as a transfer URL, which the framework needs to be able to open, whether POSIX file path or an xroot URL. Like with SAM, the framework should be able to do the next file request - it's just a REST API, using a JSON document supplied by Workflow to the job.}
The WMS needs to be able to supply framework configuration parameters such as input file(s) or random number seeds to each framework application instance, which it should do using the framework’s configuration language.  To minimize errors, these parameters should be self-describing and validatable, and so:

%4
\item the framework should provide a suitable API so that algorithm writers can ensure their required parameters are self-describing and validatable.

\item The framework should provide the concept of parameter sets that are nestable.  The set of all parameter sets that define a framework application instance should be identifiable, referred to here as a FrameworkConfigID.  Tracking that identity is one of the ingredients necessary to ensure scientific reproducibility.  However some parameters do not (and should not) change the algorithmic results, such as a debug print flag.  Independent of the state of such a flag it should be possible to define equivalence between FrameworkConfigIDs.

%5
\item The framework configuration system needs to have a robust persistency and versioning system that makes it easy to document and reproduce previous results.  It must be possible to create, tag, check-sum, store and compare configurations.  This configuration management system should be external to the framework or data files so that configurations can reliably be reused and audited.  

Further fundamental requirements related to the framework configuration to guarantee reproducibility are:

%6
\item The resulting state of the configured framework and its components should be deterministic and precisely reproducible given a set of environmental conditions which include the available hardware, operating system, input data, etc.  

%7
\item the ensemble of the framework+environmental conditions must give reproducible results.  

Following on from the discussion of supporting fast analysis R\&D:

%8
\item it is desirable that it should be possible to only configure those framework components required for a particular data processing use case.  

This generates a further requirement that 

%9
\item it must be possible to derive the input data requirements for any algorithm, in order to define the sequence of all algorithms needed for a particular use case.
\end{itemize}
 
\subsubsection{Conditions Configuration}

In addition to the Trigger Record data, data processing requires access to other data from various sources, for example slow controls, detector status, beam component status.  Such data is referred to generically as “conditions data” and also includes e.g. detector calibrations and any data external to the Trigger Record.  The time granularity or “interval of validity” of this data varies by source and is typically of much coarser granularity than individual Trigger Records, e.g. calibrations may be valid for months of data taking.  Meanwhile there are often several versions of conditions data and correlations between conditions data is not uncommon, making the coherent management of conditions data a challenge in itself.  For this reason, conditions data management should be external to the framework.

\todo{justify why this can't be done with cvmfs?}
Access to the external conditions data should preferably proceed via REST interfaces that support loose coupling of the framework and the external conditions management system.  As conditions data may need to be transformed from its persistent format into a format required by an algorithm, and as multi-threading makes the cache validity of conditions data complicated: 

\begin{itemize}
%18
\item the framework must provide a thread-safe conditions service that is a single point of access to conditions data.  

%19
\item The configuration of the framework conditions service should ideally be via one configuration parameter (a global tag). 

Developers must not hard-code conditions data in their algorithms, although 

%20
\item it must be possible to override a subset of global tag configured conditions for testing purposes.  

Developers usually find it convenient if such alternative conditions payloads can be provided outside of the main managed conditions system, e.g. via a local file.
\end{itemize}

\subsubsection{Concurrency} % Need to distinguish production and analysis
Many aspects of DUNE data processing are well suited to concurrency solutions and {\bf the framework should be able to offload work to different compute architectures efficiently}, facilitate access to co-processors for developers and {\bf schedule thread-safe and non-thread-safe work}.  {\bf It must be possible to derive the input data requirements for any algorithm in order to define the sequence of all algorithms needed for a particular use case, and it must be possible to only configure those components required for that use case.}

The arrival of concurrency and heterogeneous architectures has added a further level of complication for framework designers and developers alike.  Much of the existing code-base still relies heavily on serial programming for CPU architectures, meanwhile the co-processor market is evolving rapidly resulting in a diverse hardware landscape.  Both multi-threading and co-processors present challenges for both frameworks and developers.  
\begin{itemize}
%10
\item It is therefore desirable that the framework should help facilitate the use of multi-threaded data processing and facilitate access to co-processors in an efficient manner. 


Multi-threading presents additional, well-documented challenges, particularly given that many important libraries are not thread-safe.  Summarising briefly: algorithmic code, including sub-algorithms, should be thread-safe and must declare their compatibility to the framework.

%13
\item The framework must be able to schedule thread-safe and non-thread-safe work appropriately.  

Thread-safety implicitly includes a general requirement on developers that algorithms and their sub-algorithms do not store state information in a thread-unsafe manner.  Further, data exchange should be done in controlled ways to ensure thread-safety, e.g. via some service which manages transient data - again the implementation of this challenging aspect is left to the framework designers.

\end{itemize}
\subsubsection{Provenance} % Does it need a separate subsection


The need for transient data products is driven by the large Trigger Record sizes that can be encountered in the DUNE data, and whose transformation may be required, but for storage considerations are not written out.

The use of highly parallel and heterogeneous computing environments leads to additional provenance requirements regarding the execution ordering and computing architectures on which the algorithms were executed.  The need for this information arises because of the possibility of different computing architectures producing different results, and of accelerators and other computing offload mechanisms producing different results than serially executed or non-accelerated codes (i.e. GPU accelerated code producing different results than the same code executed on the host processor).  Therefore 

\begin{itemize}
%15
\item the framework must provide full provenance information and all of the metadata required to ensure reproducibility.  

This will include the computing architecture, including any specialized hardware used, on which the application is run as well as the runtime environment, execution model and concurrency level that the application used.  It is likely that a full picture of the necessary metadata will also require information only known to the workflow management system.  In such a complex environment, it is highly desirable that the framework provide support to allow the effect of configuration changes and computing environments to be easily understood.

\end{itemize}

\subsubsection{Externals including Machine Learning}
Machine learning is already heavily used in analysis of ProtoDUNE data and the framework should give special attention to machine learning inference in the design, both to allow simple exchanges of inference backends and to record the provenance of those backends and all necessary versioning information.  Finally, the framework should be able to work with both Near Detector and Far Detector data on an equal footing, and within the same job.

DUNE will use several libraries outside of the framework software stack, and it will also be necessary to record the precise versions of all of these libraries to guarantee reproducibility of the physics results.  It is noted that containers (docker et al) could potentially make provenance tracking easier in this respect.  

%% ANDREW Added
%(16a) The framework must provide deterministic results when run with different levels of concurrency.
%% and
%(16b) The framework must persist data in the same ordering when run under differing levels of concurrency.


One important source of external libraries relates to machine learning, and machine learning inference is expected to play a significant role in all stages of data processing including analysis.  

\begin{itemize}
%17
\item The framework should give special attention to machine learning inference in the design, both to allow simple exchanges of inference backends and to record the provenance of those backends and all necessary versioning information.


%Now move on to discussing "the" framework requirements, separate section? 

\end{itemize}


\subsection{Analysis\hideme{Norman/Laycock - still needs more}}

Based on the recommendations of the \dword{hsf} panel experts, the analysis use case is not required to be supported by the same software framework as that used for production.


Analysis includes the extraction of oscillation parameters, but is not limited to that, encompassing, as a minimum, comparisons between data and Monte Carlo, extraction of calibration and detector performance parameters, cross-section measurements, measurements of atmospheric, solar, and supernova neutrinos, and searches for non-standard phenomena.

The event-by-event paradigm is not a good match here. Spectra are filled by looping over neutrino candidates, or any other particle candidate, but then may undergo substantial processing in their own right. For example, the main work of an oscillation fit is the evaluation of many different combinations of oscillation and nuisance parameters.  While slices provide sub Trigger Record control flow, particle candidate control flow ignores the Trigger Record structure entirely.

\begin{itemize}
% #33
\item It must be possible to define arbitrary units of execution that are independent of Trigger Records.  It must be possible to correlate these units to Trigger Records for exposure accounting, and experiment conditions.


%34
\item The framework should make minimal assumptions about the data model, i.e. it should not assume an event-by-event paradigm.
%%---

%35
\item Analysis must be able to use particle-candidate-based control flow, without any constraints arising from the event-by-event paradigm.


Analysis work will be undertaken by a large number of collaborators, with varying levels of experience. In many cases, rapid feedback and iteration will be required to make progress:

% added, changed #38
\item The analysis framework should have a low entry-level in terms of software expertise.

Experience shows that oscillation fits accounting for large numbers of systematic uncertainties are resource intensive, while analysts will likely only have access to modest local resources for prototyping and development.  Therefore the framework should make scaling and concurrency transparent both to the analyst and the developer as far as possible.  The use of declarative analysis techniques should be strongly encouraged to support this even when co-processors (and low-level implementation) changes.

% adapted 39
\item The analysis framework should easily scale from local resources such as a laptop, up to multi-node compute at an HPC.

It is noted that MPI-enabled frameworks written in python already exist and would be a good match to the above requirements.

Analysis files, of course, are derived from the data-processing framework files, and it must be possible to reconstruct this history. Due to the very large number of Trigger Records expected to be summarized in a single analysis file, the size requirements, and the fact that per Trigger Record information remains available in the parent files, we require:

%40
\item Analysis files must record their parent framework files, but no Trigger Record provenance is required.  The full provenance information need not be retained in analysis files as this could easily become larger than the data itself.

One common and insidious class of mistakes is errors in exposure accounting and normalization. This is also a problem that is entirely solvable at the technical level.  Each individual Trigger Record (beam spill or other trigger) has exposure associated with it, whether POT or livetime or both. When filling a summary histogram from processing Trigger Records, the exposure should be calculated and stored as an integral part of the histogram, and operations between histograms should take correct notice of the exposure, e.g. ratio of one large exposure sample to a smaller exposure sample should produce a dimensionless ratio that has allowed for the differing exposures.

%41
\item The framework must have native support for exposure accounting (POT and livetime), so as to make errors of this sort difficult.

All but the simplest analyses require a treatment of systematic uncertainties. There are three main technical means by which these systematics can be introduced. The most common, and most convenient, is reweighting. For example, the effect of various cross-section and flux uncertainties may be encapsulated by applying weights to Trigger Records of certain categories, to increase or decrease their representation in the final spectra. Secondly, Trigger Record data may be shifted. For example, an energy scale uncertainty may be most conveniently represented by rewriting of Trigger Records to increase or decrease reconstructed energies by a certain amount. Finally, the least convenient method is alternate simulation samples. The profusion of files requiring processing and bookkeeping makes this a heavyweight option, but in the case of uncertainties early in the analysis chain with complex effects, it may be the only way to handle them accurately. The treatment of systematics is cross-cutting across all analyses, it is important it is handled correctly, and the framework is able to offer substantive technical assistance.

In addition to being able to handle multiple input data streams:

%42
\item The analysis framework should provide some means of cross-referencing (labelling) multiple input streams to correlate them in order to facilitate evaluation of systematic uncertainties.

For oscillation analysis, it will be important to work with both Near and Far detector data. Whether in an explicit joint fit, or where extracting constraints from the ND to apply to the FD analysis, there must be a uniformity in the treatment of various systematics. In general, experience gained with the Near Detector (where the majority of analysis work is likely to happen) should be transferable to the Far Detector.  This re-emphasises the importance of the framework making minimal assumptions about the Data Model.

%43
\item The analysis framework should be able to work with both ND and FD data on an equal footing, and within the same job.
%%---


\end{itemize}



\section{Timeline}

Near term vs long-term plans.  As the production use case can be taken care of by a traditional software framework, the focus in the near term should be to continue as we are and thereby learn more about the physics use cases and ultimately the needs of DUNE.  The long-term plan should not be saddled with supporting short-term needs for ProtoDUNE.  Fermilab has recently approved a Lab Directors Research and Development project to start the process of designing an improved framework capable of satisfying many of these requirements. 

\todo{HMS 3/13 Check what I said about LDRD}

%\todo{I give up at this point ignore rest of chapter}
% {\hideme
% \subsection{Missing functionality \hideme{Norman/Laycock - needs more}}

% The DUNE production framework will ultimately need to handle much more demanding trigger record sizes than ProtoDUNE and this will be the main development challenge.  In addition, major development work will be needed for:

% \begin{itemize}
%     \item one
%     \item two
% \end{itemize}

% For analysis, early investigations into an MPI-based framework (CITATION NEEDED) already showed promising results.  The compatibility of DUNE analysis to HPCs suggests that this would be an efficient way for analysis computing resources to be provisioned to DUNE.

% \subsection{Plan to go forward \hideme{ Norman/Laycock -needed }}

% Development plan.
% }
%This plan was further reviewed by the HSF, maybe.
%
%\subsection{HSF recommendation for a path forward}
\end{document}