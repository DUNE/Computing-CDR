\documentclass[../main-v1.tex]{subfiles}
\begin{document}

\chapter{Frameworks}\label{ch:fworks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{xyz}
%\label{sec:fworks:xyz}  %% fix label according to section

\dword{dune} presents a unique challenge for data analysis and data processing software frameworks. 
%For \dword{dune} to be successful, we will need to be able to extract high fidelity physics information from the reconstruction, processing and filtering of large datasets, which are organized as variable time-series.   
\dword{dune} has an ambitious physics program that spans numerous physics topics including precision neutrino oscillation measurements, searches for proton decay, sensitivity to nearby SuperNova explosions, and more. Moreover, these physics interactions occur at vastly different timescales, from nanoseconds to 100s of seconds, and software frameworks that are fully capable of adapting to these varied timescales efficiently will be very important. Furthermore, accomplishing timely results will require that data from the \dword{dune} far and near detectors  be efficiently processed using modern computing techniques. Given evolving computing architectures, traditional \dword{hep} software must be adapted to run on new generations of advanced and accelerated computing resources such as \dword{hpc}, \dwords{gpu} and other novel computing architectures.  

Development of this type of modern computing pipeline and analysis structure will require reoptimization and extensions of the existing software frameworks used by \dword{dune}, and shared to a large extent by the greater neutrino community to which \dword{dune} and its computing ecosystem belong.
In this Chapter we provide a brief overview of our existing code base, followed by a description of our process for designing the optimal frameworks for data processing and analysis that will be needed by the time \dword{dune} begins operations.  

\section{Defining a Framework}\label{sec:framework:def}

\subsection{The Data Atom}

Modern \dword{hep} experiments generate large volumes of primary detector data, as well as auxiliary and ancillary data from secondary systems. Along with detector data, vast amounts of simulation which mimic these detectors and their responses provide estimates and predictions for the different physics processes that may be observed in the experiments.  The management, flow, transformations, analysis and interaction of these data and their algorithms require a well defined framework for governing and sequencing these tasks.  In particular in traditional \dword{hep} analyses, data processing and analysis frameworks are designed to provide "event loops" which allow for experimenters to step through the data and simulation in a manner that can break up the information into discrete chunks which represent physical processes.  These discrete chunks are referred to as "data atoms" and contain a fundamental assumption that they can be treated as independent units of work.

Historically in \dword{hep} experiments the data atoms have been associated with well defined external time structures which govern the data production mechanism and/or collection process.  For collider experiments these are often associated with a particle accelerator beam or bunch crossing, while for fixed target experiments they have often been associated with pulsed beam extraction to target stations (often referred to as a "spill"), or substructure within the beam spills defined by the data acquisition and triggering systems (i.e. the data atoms are the individual triggers).  

In the case of the \dword{dune} physics mission and the resultant system designed to accomplish that mission, the definition of these data atoms is not as simple or as static. %as it has been historically in other experiments.  
While neutrino beam interactions have natural timescales of nanoseconds, the timescale of supernova neutrino bursts, proton decay searches, and long-lived, BSM particle searches span up to 100s of seconds. The nature of these varying times scales within the \dword{dune} physics program results in data atoms which vary in size and structure, and may be spread across numerous files and data structures. 
Furthermore, apart from the neutrino beam interaction case, the data production mechanism is not provided by a deterministic external source and is random in nature.
\dword{dune} data atoms could represent a) an arbitrary time window representing a time period of interest b) a time window representing the drift of ionization across the detector volume c) a spatially and temporally isolated region of the detector readout d) a sub-region of interest within a spatially/temporally distinct portion of the detector or e) other segmentations of the \dword{dune} detector readouts corresponding to potential physical signatures with time structures defined by the physical process. Having a framework that can adapt to varying time structures, multiple input files, and data structures is a very important feature for \dword{dune} to accomplish its vast physics goals.

\subsection{Key Software Framework Concepts}

%In this manner, the \dword{dune} data
A software framework is a software structure and engine that can ingest and apply transformations and filters to the individual data atoms regardless of their underlying nature and related time structure.  The framework has a responsibility for controlling and managing the I/O associated with ingesting the data atoms, for sequencing and scheduling the algorithms and transforms that run on the data, and for providing logging, provenance and other bookkeeping tasks associated with describing and enumerating how the data was handled and new information derived.  

To these ends, the framework provides mechanisms (such as "plug-in" structures) for experimenters to develop modular algorithms which can be included or excluded from a given analysis chain and be run either serially or in parallel depending on the data context.  The framework also provides mechanisms for data to be passed between different algorithmic portions of the data processing and analysis chain while maintaining the data's coherence and integrity from the standpoint of the computing platform.  In some framework implementations this data passing mechanism is provided by a {\it data store} which includes controlled forms of data access and locking.  
% PJL If it's passing data around in memory then it's transient?
%Data stores can be used for both transient and long term information passing.  
Frameworks which use these types of methods for data passing need to then additionally manage memory footprints of the stores to ensure that they can operate within the technological memory capacities provided by the platforms on which the codes run.  Other frameworks can provide less rigid data pipelines for information passing which can be advantageous when dealing with large data streams and which can better accomodate ingress and egress data paths with respect to the bulk memory footprints, but which may not be compatible with certain types of parallel processing and parallel algorithms, or with algorithm scheduling systems.

The other key feature that modern frameworks provide, is the ability to configure and re-configure the content and flow of the execution modules without the need to rewrite the underlying algorithms.  This aspect of a framework as a configuration driven entity is key to their flexibility and their ability to satisfy the needs of the many different and varied analyses that are proposed for \dword{dune}.  Without a robust configurable framework, different data processing tasks and analysis chains could be spread across a wide variety of codes, likely with significant duplication, which could severely hamper the collaboration's ability to understand the derivation of results and ensure their integrity during scientific reviews.  A robust configuration driven framework system allows for both minor changes to individual parameters, or perturbations on those parameters to investigate the data or simulation's response to those changes, and allows for reordering, restructuring and inclusion/removal of algorithmic blocks without the need to edit or recompile the core physics codes.  This in turn promotes code stability, reuse, and facilitates modern code design and debugging principles.  

Another key concept that defines modern data processing frameworks, are their construction and organization as advanced state machines.  In contrast to older, linear execution techniques, this design allows for the execution of more complicated workflow topologies, parallel analysis chain topologies, and dynamic execution models.  This state machine organization and behavior also allows the frameworks to emulate better the behavior of other systems that are used throughout  \dword{hep}, such as those encountered in the data acquisition systems realm, and to adapt more readily to parallel and asynchronous data access systems where the coherence of the system's ``state'' can be monitored and maintained between framework controlled state transitions (i.e. between configuration, initialization, algorithm execution, data serialization, and finalization stages).  

It should be noted that in modern framework definitions and designs, there is typically an interplay and hand off between the framework which is executing the algorithm code, and the higher level \dword{wfms} layers which are responsible for the batch scheduling and delivery of the framework ``jobs'' to compute resources at different computing sites or on different types of computing hardware.  Modern frameworks are typically designed to be aware of this macro level scheduling layer, and will often provide hooks to allow for the management layers to either transmit or receive information from the framework jobs. Information regarding framework configuration and various types of diagnostics can help both the framework and the \dword{wfms} layers more efficiently execute their missions, e.g. a framework will often provide monitoring diagnostics regarding the overall progress of the job which then allows for the management layer to stage queued work or initiate data transfers between sites.  Equally, the \dword{wfms} may provide ``late binding'' configuration information to the framework which allows it to determine at runtime sources of input data, or locations for output data which may be site specific or coordinated by the higher level management tools.

Lastly, modern frameworks as used by \dword{hep} today are also designed and responsible for adapting to  heterogeneous computing resources.  Modern frameworks must, due to the push towards exascale computing platforms, be able to execute their codes or bind/link their codes across not only different operating systems, but across different hardware architectures.   In the case of \dword{dune} this will be an enabling technology that allows \dword{dune} algorithms to run on platforms ranging from commodity desktop/laptop systems, to large scale grid computing centers, and across the exascale era leadership computing facilities being built by the \dword{doe} and other major national and international computing centers. These facilities' architectures will span from the classic x86 CPU architectures to advanced, many-core GPU and AI/ML tuned accelerator systems.  The framework's ability to deal with this diversity is a requirement from \dword{dune} and is discussed in detail in the following sections.

\section{Current status}\label{sec:framework:status}

Currently a set of frameworks have been used for the simulation of the \dword{dune} detectors, and processing of data from the \dword{protodune} detectors.  These frameworks have been developed or adopted based on the immediate needs of the sensitivity studies, simulation studies and \dword{protodune} running.  These frameworks are based off of earlier frameworks that have been used in the neutrino community and reflect in part the features that are needed for long and short baseline neutrino analysis.

For the \dword{fd} simulation and reconstruction efforts, the \dword{protodune} detectors and \dword{ndgar} studies, the \dword{art} framework which was developed at \dword{fnal} is the primary framework being used throughout the collaboration.  The \dword{art} framework, was originally developed based off of the CMSSW framework used by the CMS experiment, but was designed to meet the specific needs of the neutrino and muon science communities.  The \dword{art} framework was designed as a multi-experiment framework starting in 2011~\cite{Green:2012gv}, and is currently used by 11 different major experiments in the \dword{hep} community including \dword{dune}, NOvA, MicroBooNE, ICARUS, SBND, Muon g-2 and Mu2e.  It is developed, maintained and supported by \dword{fnal}'s \dword{scd} and through a stakeholders committee consisting of the experiments using the framework.

The framework provides the data processing loop, manages memory, interfaces to I/O tools, defines uniform mechanisms for defining, associating, and persisting data products, provides a uniform mechanism for job configuration, stores job configuration information in its output files, and manages messages, random numbers and exceptions.  The framework is highly modular and common modules which are shared across experiments have been developed for common infrastructure components such as the accelerator beam information systems at \dword{fnal}, various data management systems and data cataloging systems, and database systems for calibration and conditions data access.  Use of the \dword{art} framework has allowed for \dword{dune} collaborators and other experimenters to quickly interface with \dword{fnal} facilities, and with other facilities hosted by \dword{cern} and other sites, in a common and consistent manner.  The framework also allows for dynamic (runtime) configuration of the code modules, which has allowed for additional common code libraries to be developed and run on top of the \dword{art} base. 

In particular, the \dword{larsoft} toolkit is a collection of \dword{art} plug-ins and associated algorithm code, configuration files, static data such as geometry specification files and photon visibility maps which has been developed for the liquid argon TPC detector community.  \dword{larsoft} provides the interface to neutrino event generators such as \dword{genie} and cosmic ray generators and simulations such as \dword{corsika} and CRY,
detector simulation via \dword{geant4}, custom simulation and reconstruction software, event displays and tutorials.  Experiment-specific metadata and configuration database plug-ins assist in batch workflow organization.  Like \dword{art}, \dword{larsoft} is supported by \dword{fnal}'s \dword{scd}, and through collaboration and contributions from participating experiments.

For the gaseous detector simulations, the \dword{garsoft} software package is used.  \dword{garsoft} is patterned on \dword{larsoft} as a layer of common algorithms which runs on top of the \dword{art} framework.  It provides a robust toolkit for simulating and reconstructing data from the \dword{ndgar} concept detector or for other gaseous argon detectors.  Like \dword{larsoft}, it provides interfaces to event generators and \dword{geant4}, custom simulation and reconstruction software, and event displays.  It also provides simulation and reconstruction for \dword{ndgarlite}.  Unlike \dword{larsoft}, \dword{garsoft} is written, maintained, and supported only by the \dword{dune} collaboration and is not shared with other experiments in the neutrino or \dword{hep} community.

In addition to the \dword{ndgar} software, the \dword{dune} collaboration's near detector groups have developed different tool suites for simulation and analysis of the proposed near detector designs.  The \dword{ndlar} software, which represent a group of standalone tools for simulating and reconstructing pixel-based \dword{lartpc} data.  These tools and toolchains have been developed for simulating and analyzing \dword{singlecube} prototype data.  The \dword{sand} software efforts are based off of collaborator's experience with the \dword{kloe} detector and its software.  Portions of the \dword{sand} software are being used to provide simulation for the magnet and calorimeter systems, while new software is currently being developed for the \dword{3dst} and other components of SAND.  These software stacks are specifically being developed with maximum flexibility due to the design stage of the SAND concept.  This software flexibility is important at this stage in the design in order to allow studies of different detector designs which can be used between the \dword{3dst} and the calorimeter.  These tools are currently only used by the near detector group and have an independent software structure from the long baseline groups.

For higher level analysis tasks, sensitivity studies, and event selection, \dword{dune} is currently leveraging the 
\dword{cafana}
framework.  \dword{cafana} is a high level framework based on the \dword{root} analysis software stack, which is designed to work with \dword{root} TTree's (ntuples).  \dword{cafana} is designed to provide bookkeeping facilities for neutrino flux information and exposures, which are needed for long baseline oscillation measurements and for short baseline cross section measurements.  \dword{cafana} is designed to work with output from data that has been processed with \dword{art} and \dword{larsoft} and provides a more interactive environment for experimenters to explore the data, by using the \dword{root} scripting and interpreter interfaces, with specific \dword{cafana} libraries and functions.  \dword{cafana} provides facilities for high level event selection, normalization of distributions, re-weighting distributions,  and more.

The distinguishing features of the two frameworks, \dword{art} and \dword{cafana}, are that \dword{art} works at the individual event record level, while the \dword{cafana} framework allows for high level event selection but operates primarily on the resulting ensemble level distributions.  

\section{Framework Requirements \hideme{Laycock/Norman - needs update}}

%PJL Lots of repetition here
%The current \dword{dune} use of different frameworks and common algorithm code stacks built upon these frameworks, has led to the question of whether this software has the required features and scalability to serve the needs of the full \dword{dune} science program.  In particular it has been recognized that the full \dword{dune} detector systems, even when scaled back to only the first far detector module, represent an event data size that is orders of magnitude different from the types of data that are analyzed in the \dword{hep} community today.  Moreover, it has been recognized through our use of the current software tools on the analysis of the \dword{protodune} Run 1 datasets, that our current tools do not behave the ways we had expected and that there are scaling issues with the performance of the software and with the interfaces it has to the different algorithms layers.

\todo{PJL do we want to say more on what didn't work for ProtoDUNE?  If so it belongs in Current Status.}

% and here
%We also have recognized that \dword{dune} exists at a new point in the evolution of the global computing landscape, where a move towards high speed accelerator technologies (GPUs), and AI/ML tuned hardware architectures have proliferated across current and planned scientific computing facilities.  These heterogeneous computing platforms are the future the \dword{hep} computing and of \dword{dune}'s focus when accessing our needs.  Modern \dword{hep} software frameworks are in the process of addressing this increasing heterogeneity of computing architectures that are redefining the computing landscape, and this work is being pursued by the \dword{hsf}~\cite{Alves:2017she, Calafiura:2018rwe}. The \dword{dune} collaboration has specifically been involved with the \hsf} and has been participating in these efforts in order to align our \dword{dune} development efforts and planning with the overall plans and work that is occurring with other frameworks in \dword{hep}.

\subsection{Requirements Process} \label{sec:req:proc}

Given the unique challenges that \dword{dune} data pose, in 2020, the collaboration assembled a task force to examine and define needs and requirements for the \dword{dune} software frameworks that are needed to accomplish the core physics missions of \dword{dune}.  The taskforce was charged with identifying ``physics use cases''  which could then be translated into software requirements that could be imposed on the current framework system, and applied to any new software systems\cite{bib:docdb21934}.  The collaboration then approached the \dword{hsf} who assembled a panel of software framework experts from various experimental backgrounds to review these requirements and assess their compatibility with existing \dword{hep} software. The report \cite{bib:docdb24423} produced by this panel was discussed in a workshop involving the panelists and task force members, which resulted in a followup workshop to address the outstanding issues, mostly around the topic of concurrency and mapping \dword{dune} software and data processing to \dwords{hpc} and leadership scale computing facilities.  The findings of the panelists, and the summary of the concurrency workshop \cite{bib:docdb24426}, have been incorporated into the \dword{dune} software framework requirements documented here.  In particular a number of issues unique to \dword{dune} were identified which must be addressed as \dword{dune} and its software moves forward.  We present these in the following sections with details regarding their need and their impact to the \dword{dune} computing model.

\subsection{Summary of Software Framework Use Cases} \label{sec:fworks:use_cases}
\hideme{HMS 3/13 I added a table - probably need a row for a single ND event}

The complete set of use cases that was identified for \dword{dune} has been enumerated and is found in Table.~\ref{tab:fworks:use_cases} and detailed in \cite{bib:docdb21934} as well as the \dword{hsf} report of section~\ref{sec:req:proc}.  


%The use cases for \dword{dune} are summarized in table \ref{tab:fworks:tasks}

\begin{dunetable}
[Summary of computing resources needed per trigger record]
{l r r r r r r }
{tab:fworks:taskstr}
{Summary of resources needed per trigger readout for compute intensive tasks in \dword{protodune} and the \dword{fd}. These numbers assume that reconstruction will load individual subunits such as \dwords{apa} into memory one at a time, possibly in parallel across multiple cores. }
Use case	&memory &	input size &	output  size 	&	CPU time 	&	input/core  	& cores/readout		\\
units	&GB	& MB	&MB	&	sec	& MB/s	&		\\

Simulation+reco	&		6	&	100	&	2000	&	2700	&	0.00	&	1 to 150	\\
FD reco	&	4	&	80	&	4000	&	600	&	0.13	&	up to 150		\\
SNB reco	&	2	&	3.2$\times10^8$	&	? 	&3-10$\times10^8$		&	0.13	&	10-30,000		\\
%tuple creation	&	3	&	4000	&	1	&	100	&	40.00	& 1		\\
%calibration	&	3	&	4000	&	1	&	100	&	40.00	& 1		\\
%Parameter estimation	&		&		&		&		&		&			\\
\end{dunetable}
\todo{PJL 03/16 - Table needs SNB output size and a line for an analysis that we want to run on HPCs, i.e. "the whole thing with nuisance parameters"}

One striking feature of the use cases that \dword{dune} needs to support is the diversity in scale of the data atom.  Somewhat counter-intuitively, although the raw trigger records are themselves very large, the data atom (a single \dword{apa} or \dword{crp}) is relatively small and compatible with the same high-throughput computing workflows adopted by most \dword{hep} experiments.  Meanwhile at the analysis level although the quantity of data related to a single trigger record is small, nuisance parameter extraction requires correlations across all trigger records in an analysis dataset, which is the effective data atom.  This is a very good fit to high performance computing, particularly if the analysis framework can take advantage of many \dword{hpc} nodes at the same time. Experience from the \dword{hsf} panelists encouraged \dword{dune} to separate the analysis and production use cases when considering the software framework design.


\subsection{Unique Software Framework Requirements for DUNE}
\label{sec:fworks:urgent}

While section \ref{sec:fworks:use_cases} provides an overview of use cases, certain unique use cases drive the needs of the \dword{dune} software framework to be fully compatible with the physics missions.   We summarize these driving requirements here as a summary of the major work that needs to be executed for \dword{dune} to have a software framework capable of supporting its physics program.

% Table \ref{tab:fworks:tasks} summarizes the dominant use cases for the large scale reconstruction and simulation framework.

Driving use cases and consequences on framework development:


\begin{itemize}
\item Far detector partial region simulations and subsetting of temporal and spacial data.
\end{itemize}

The actual simulation of physics processes in the \dword{fd} are likely to span only a subset of a detector module as individual interactions are confined to a reasonably small volume and a prompt temporal footprint.  However, without substantial effort, to accurately simulate the detector response, all of the simulated hits regardless of locality have to remain in memory at once.  This memory footprint is substantial and presents difficulties for scaling to current and future computing platforms.  The framework will need to be able to handle the management of these simulations and in particular will need to be able to subset them efficiently and effectively ``page'' them in and out of the active memory in order to propagate information between stages.  Simulations of a 2x6 \dword{apa} region in the \dword{fdhd} have already been demonstrated, but this needs to be extended and made a native function of the framework's memory management and handling systems.  This is also an area where auxiliary detectors, and in particular the photon detectors may add substantially to the memory footprint as discussed in section \ref{ch:use:pd}. 


\begin{itemize}
\item Far detector partial region reconstruction and subsetting trigger records for data processing
\end{itemize}

Reconstruction algorithms running within the \dword{art} framework, are already running into memory limitations when processing the 6 \dwords{apa} \dword{protodune} data trigger records.  While most \dword{fd} Trigger Records, whether initiated by cosmic rays, beam neutrino interactions, or  atmospheric neutrino interactions, will result in activity in only one \dword{fd} module, there are still extensive air shower events and other physics events, even at the 4850' level of \dword{surf}, which results in correlated activity spread across multiple modules.  Processing data from a full \dword{fd} module, with hundreds of thousands of channels and thousands of samples/channel will require sophisticated memory management to handle not only ingesting the data, but working with it through the different algorithmic transforms that are required for the digital signal processing, the reconstruction tasks, and for writing out the resulting information.  There is however natural parallelism in the \dword{dune} detector corresponding to the APAs.  A framework designed to operate with knowledge of this parallelism, would be able to work with the individual readout subunits such as the \dwords{apa} or \dwords{crp} and process them either individually, sequentially, or in parallel in such a manner as to manage the total memory footprint.  In contrast current frameworks, such as \dword{art}, are not designed in this manner and can not natively handle this type of subsetting and scheduling.

\begin{itemize}
\item Temporal and spatial ``stitching'' of readout trigger records into new extended windows
\end{itemize}

  In the event of a supernova burst, or other multi-messenger or long time duration events, the data from multiple trigger records needs to be combined across readout boundaries to form complete views of the interactions
  and, perhaps more crucially, to avoid edge effects in the \dword{fft} (see section \ref{sec:intro-introduction}).  In the case of a SNB Trigger Record, the data from ten's of seconds of readout will be 100's of TB in size and may have thousands of physical interactions spread across the active volume.  Proper analysis of extended events like this require that data from different portions of the underlying DAQ readout be ``stitched'' across boundaries in both the detector spatial dimensions and also in time.  However, current \dword{hep} frameworks make the implicit assumption that data atoms are independent of all other data atoms and that the temporal ordering of data atoms can be ignored when scheduling data processing tasks.  To overcome this, the framework that \dword{dune} uses will need to handle data that has a well-defined temporal ordering, and combine data from adjacent readouts, taking into account overlap regions, effective sidebands, and duplication of data products.  This is a highly non-trivial task and one for which R\&D is required for developing the methods for scheduling these types of merger operations within the context of a parallel or distributed computing environment.

%   They will be small tracks (“stubs”), of order of tens of wires hit in each plane, distributed through the detector and in time during the long Trigger Record.  A convenient analysis workflow will save regions of interest to smaller files containing only data needed to analyze the small tracks and not the large amounts of waveform data containing only electronics noise and radiologicals. 

\begin{itemize}
\item Contextual switching of primary data atom types for driving event loops
\end{itemize}
  
  In contrast to the far detector supernova readout, the \dword{dune} near detector due to its proximity to the target station, high fluxes, and cavern siting, will have many neutrino induced interactions within or crossing the detector volume during each LBNF spill window.  The Gaseous Argon Near Detector Component (ND-GAr) expects approximately 60 overlaid interactions per spill.  Most of these interactions will originate in its calorimeter, which has fast timing capabilities that can be used to disambiguate the interactions.  Disambiguating these interactions will allow for a single trigger window to be effectively unfolded into the particle content of each interaction.  Similarly, the \dword{lartpc} near detector is expected to have on the order of 20 interactions per spill which can also be disambiguated and separated out.  From the framework perspective this subsetting of a data window into what has been colloquially referred to as ``slices'' changes the primary context of items that need to be looped over and analyzed.  The \dword{dune} framework will need to be able to handle this type of a contextual switch, so that it can run appropriate reconstruction and identification algorithms over each of the interactions, instead of over the original readout window.  In much the same way that current frameworks are not designed to handle ``stitching'' of data atoms, current frameworks similarly are not designed to break data atoms down into smaller atoms while retaining the required bookkeeping information, and then looping and scheduling algorithm modules over these.

%  and there by allow for a single trigger window to be un particles belonging to different Trigger Windows.  The liquid-argon TPC near detector will have order of 20 interactions per spill.  Some downstream analyses will benefit from and expect upstream analyses to divide a Trigger Record’s data into subsets based on classification algorithms that are intended to separate one interaction from another.  These physics regions of interest, usually called “slices”, are then what physicists expect to use as a unit of execution, i.e. they loop over slices in their analysis code.
%\end{itemize}



%\subsection{Data and I/O layer}

%As noted the discussion on raw data processing, the DUNE software framework must have the {\bf ability to  change its unit of data processing} from single planes to supernova readouts with extreme flexibility and ensure that memory is treated as a precious commodity.  

%% This is really talking about slices, covered in Analysis
%It must be possible to correlate these units to DUNE data-taking "events" for exposure accounting, and experiment conditions.


%{\bf Partial reading of data} is another strong requirement needed to keep memory usage under control.  There must be {\bf no assumptions on the data model}, {\bf the framework must separate data and algorithms} and,  further, {\bf separate the persistent data representation from the in-memory representation} seen by those algorithms.  DUNE data is well suited to massively parallel processing compute architectures, so the framework will need to {\bf have a flexible I/O layer supporting multiple persistent data formats} as different formats may be preferred at different stages of data processing.  The sparsity of DUNE data also imply that compression will play an important role in controlling the storage footprint, especially for raw data, and the sparsity of physics signals further emphasize support for data reduction (skimming, slimming and thinning) in the framework.  The framework {\bf needs to be able to handle the reading and writing of parallel data streams}, and navigate the association (if any) between these streams with a {\bf critical need to allow experiment code to  mix simulation and (overlay) data}.

%37 - this could be tied to req #1, separation of data and algorithms?
%--REQ ON DUNE, NOT THE FWK - Calibration, reconstruction, and selection algorithms must be framework-agnostic, i.e. able to run transparently in any official DUNE framework where equivalent requisite data products exist.

%% PJL 03/16 - removing this as it repeats UNIQUE
%\subsection{Memory Footprints in DUNE}
%For the purpose of documenting requirements, we have considered the known limiting cases of data processing as being the drivers for memory requirements.  While optimization of specific algorithms may be possible, we consider the generalized unoptimized cases at this time as being representative of real world data processing.
%%%%

%The DUNE \dword{lartpc} data is naturally organized into geometrically distinct regions corresponding to \dword{apa} or \dword{crp} regions.  In the far detector, the uncompressed readout is estimated at 6 GB/readout, but can be subdivided into 150-160 separate 40 MB/readout regions corresponding to each \dword{apa}/\dword{crp}.  The actual data footprint that a DUNE data processing algorithm requires to be present in core memory varies depending on the total number of \dword{apa}/\dword{crp}s on which the algorithm needs to operate on at a given time. If an algorithm needs the entire readout it requires a minimum of 6 GB of memory, while if it operates on only a single \dword{apa}/\dword{crp} then it requires a minimum of only 40 MB.  Many \dword{dune} algorithms are transformations of the base data and require a multiple of the base memory footprint to be computed (i.e. large matrix style multiplications require the base data, transformation matrix and resultant matrix).  Other operations require data from temporally adjacent readout windows and similarly require a multiple of the base data size in order to complete.

Given the potential for pressure on memory resources, a requirement on the framework is:

% PJL replaced two reqs with this, "useful lifetime" was not specific enough.
\begin{itemize}
    \item The framework must provide fine-grained control of memory, managing the memory needed by data products in the {\it data store} or its equivalent.
\end{itemize}

%28 NEED TO DEFINE "USEFUL", and what does this really mean as a requirement?
%\item Data products should not occupy memory beyond their useful lifetimes.
%29
%\item The framework must manage memory of data products in the Data Store.

\subsection{Modular Framework Design} \label{subsec:fworks:modular}

% Preamble could come first
For developers, highly modular code must be encouraged, allowing evolution or replacement of sub-algorithms that lend themselves to particular approaches.
%It is assumed that algorithmic code will be organized in “algorithm modules” and this term is used in this document.  
The codebase will therefore likely contain several alternatives for (sub)algorithms, the choice of which would depend on the available hardware.  The framework will need to run in heterogeneous and potentially dynamic environments where the availability and type of co-processors may be known late.  While the design of this technically challenging aspect is left to framework developers, it is worth pointing out the added challenge of developing algorithms for diverse hardware if the framework does not easily allow it.  Therefore:

\begin{itemize}

%2
\item the framework must separate the persistent data representation from the in-memory representation seen by algorithms

%1
\item the framework must separate data and algorithms 
\end{itemize}

For developers, fast turnaround time is crucial:
\begin{itemize}
 \item The framework should allow a rapid development setup with minimal overhead both in terms of writing code, compilation and startup time (including configuration)
%11
%\item it is highly desirable that it be possible to write algorithmic code independently of the framework.  This also helps keep the gap between analysis and production code as low as possible.

%12 - services are defined?
%\item it is highly desirable that the framework be sufficiently modular in design to allow re-use of framework services and functionality both within and outside of the framework context, as far as that is possible.

\end{itemize}

It is noted that a sufficiently advanced Configuration system (see section \ref{subsec:fworks:config}) would be able to achieve this.


\subsection{I/O Requirements for the Simulation/Reconstruction Framework}


%The first stage of processing in offline jobs, after job configuration and initialization, is reading in detector data.  Offline jobs must read in data produced directly by the data acquisition system and also data produced by Monte Carlo simulation.  Input data files may be retrieved from a persistent storage system, delivered over a network, or reside on local storage.

%% Andrew added a comment on needing to specifically support both row-wise and column-wise representations of data but this seems to be getting into the I/O implementation?  Could be used as an example rather than a requirement.


Given the uncertainty in the choice of data format:

\begin{itemize}
\item  the framework must support reading and writing different persistent data formats

%Every version of the framework must be able to read data files written by that version of the framework and all previous versions, with no loss in functionality or change in meaning of the data elements:

%22
\item The framework I/O read functionality must be backward compatible across versions.

%We do not require forward compatibility, in which data written with newer versions of the framework are also readable by older versions.  Cases in which forward compatibility is broken need to be documented as far as possible, however, as these are breaking changes.  

%Experimenters may change their minds about the contents of data products.  For example, data members may be added because they were initially not included in the design but later found to be necessary, and rather than create a new data product, an expansion of an old one is more convenient.  Framework programs reading old and new data files need to behave seamlessly if the data product has changed definition.

%23
\item The framework I/O layer needs to provide a mechanism for user-defined schema evolution of data products.
\end{itemize}
The \dword{dune} data model allows for Trigger Record data to be stored in persistable representations which are generated by customized hardware or which are optimized for specific acceleration hardware or computing systems.  As a result, the data model expects that data will have custom “packed” representations that do not conform to 32-bit or 64-bit little-endian words. Furthermore, compression of the raw waveform data will be performed in the DAQ, though some data may arrive uncompressed.  Some highly compressible data products may benefit from dedicated compression algorithms scheduled to run before output. Therefore, 

\begin{itemize}
%24
\item the framework I/O layer must provide a mechanism to register/associate and to run/apply custom serialization/deserialization and compression/decompression algorithms to data on write/read of that data from a persistable form.  

%Data products that do not have dedicated compression algorithms associated with them can still benefit from automatic compression that is enabled by default.

%25
\item The framework I/O layer should support compression on output data in a manner that is transparent to users and is configurable.  It must be possible to disable the automatic compression of output data.
\end{itemize}

Experience shows that it is highly desirable to be able to configure a maximum file size such that output files are the correct size for efficient storage and for units of data processing; currently a file size of several GBs is considered optimal.  As this requires the closure of an existing output file and creation and opening of a new file (with sensible filename) then this needs to be addressed at the framework level.

\begin{itemize}
%26
\item The framework I/O layer should allow a configurable maximum output file size and provide appropriate file-handling functionality.
\end{itemize}


% Covered in Unique requirements
%An issue that arises during data read-in and decompression and unpacking is the memory footprint used.  The DUNE Far Detector is big.  Supernova-burst (SNB) processing in the DUNE Far Detector presents unique challenges due to the large volume of data that are produced in each Trigger Record.  An uncompressed SNB Readout for 100 seconds will take about 120TB of storage for one single-phase far detector module for just the TPC wire data, and DUNE will eventually have four detector modules.  These data will be divided into smaller chunks both in time and by detector component.  For single-phase detector modules, these components are likely to be the anode plane assemblies (\dword{apa}/\dword{crp}) due to the granularity of the data preparation processing.

%In the first stage of offline processing, waveforms from the channels are de-noised and deconvolved, and pulses that are approximately Gaussian appear in the processed waveforms. Because the 100s of SNB Readout from each channel must be artificially broken into small chunks in time, there is the potential to introduce edge effects at the chunk boundaries. In order to avoid this, about 100 microseconds of data spanning the boundary on both sides must be used in processing the chunks.

%A common ratio of RAM to CPU cores on existing grids is 2 GB/core.  Memory usage beyond this results in poor performance and can lead to job eviction depending on the resource configuration.  An uncompressed DUNE Far Detector module Trigger Record will be larger than this, about 6 GB.  A supernova Trigger Record of 100s will be more than five orders of magnitude bigger again.  Clearly the framework will need to be able to act on subsets of Trigger Records, while respecting the overlap criteria noted above in order to avoid creating artefacts.

%27
%-- CRITICAL REQ The framework must be able to operate on subsets of a Trigger Record.  Specifically, it must be possible to break Trigger Records down into smaller chunks (e.g. one \dword{apa}/\dword{crp}) and be able to stitch those chunks back together.  For supernovae, it must also be possible to reuse a fraction (nominally 100 us) of the previous chunk (nominally 5ms) of data to allow stitching in time.

%Data unpacking and initial processing can be arranged to operate on these subsets.  In order to realize the benefit from operating in this way, however, intermediate data products that are no longer needed must no longer occupy space in memory.  Data products that have been written out and are no longer needed in memory are also good examples of those that can be evicted from memory, but there are also cases of intermediate products which must be flushed instead of written out.


% Leftover
%As noted in “General Considerations” (!!!!!!!), the framework must be aware of which algorithms need which data products, while the algorithmic code, by nature of its modularity and re-use/reconfigurability, is expected to be unaware of what other components may be run in the same job.  Therefore, the framework must be responsible for garbage collection, capable of freeing up memory at the earliest possible time.  Furthermore, the framework will need to respect the memory constraints imposed by the processing environment which will entail supporting partial reading of data objects into memory and potentially purging any data objects or partial data objects not immediately required.


Frameworks need to provide configurable, flexible I/O access so that experiments can control the output of their jobs in fine-grained detail.  This is needed to save on storage and also experimenter time, as smaller datasets take less time to analyze than larger ones.
 
\begin{itemize}
%30
\item The framework needs to support skimming/slimming/thinning for data reduction.
\end{itemize}

Similarly, processing and analysis is made much more convenient (or even possible) if the skimmed/ slimmed/ thinned output streams can be associated with information in other streams that may be stored separately.  Some analyzers may need the auxiliary data streams while others may not, and so a framework job that produces outputs for collaboration use would need to read all of the necessary streams.  This also allows efficient use of storage, as data does not need to be co-located in the same file to be available for processing.  In the case of writing, I/O cost can be very efficiently amortized if several output streams can be written based on one input file.

\begin{itemize}
%31
\item The framework needs to be able to read and write several parallel data streams.  Labeling of data objects across streams should be intuitive and not error prone.  Provenance information should support correlating related data objects across streams.

\end{itemize}

A common offline job need that goes beyond the 1->1 input to output data model is mixing real Trigger Record data with Monte Carlo simulation.  Monte Carlo simulation is often not sufficiently realistic, perhaps it fails to capture the time dependence of detector conditions or the generator or detector simulation simply lacks sufficient accuracy for the physics use case.  In this case, Monte Carlo simulation can be augmented by adding actual detector data, e.g. to embed single tracks or entire Trigger Records into simulated data and reconstruct them as if they were actual Trigger Records.  This can lead to a 2->many input output situation with asynchrony in both input and output. 

\begin{itemize}
%32
\item The framework needs to allow experiment code to mix simulation and (overlay) data.

\end{itemize}

%UNIQUE requirements
%36
%\item The framework must support partial reading of the persistent data and must not require reading an entire Trigger Record unless required (i.e. it must not force the entire Trigger Record to be read).
%---

%Efficient data access for typical analysis workflows is also very different to data processing using the event-by-event paradigm. It is very common to require access to only a small subset of the Trigger Record information (the variables required to make a cut, or reconstruct a quantity), and for the amount of information to vary across Trigger Records (e.g. in the case of early rejection in a physics analysis selection).





\subsection{Reproducibility and Configuration}

The reproducibility of physics results and the knowledge of how physics results were obtained is essential to \dword{dune} and to the neutrino community as a whole.  It must be possible both to replicate physics results using identical input data, or to repeat an analysis using a different set of input data with an identical sequence of identically configured algorithms. 

%\item The framework must ensure that memory is treated as a precious commodity, implying that {\bf intermediate data products cannot occupy memory beyond their useful lifetime}.  Nevertheless, reproducibility is a key requirement of any scientific tool and {\bf the framework must provide a full provenance chain} for any and all data products which must include enough information to reproduce identically every persistent data product. By definition, the chain will also need to include sufficient information to reproduce the transient data passed between algorithm modules, even though the product is not persisted in the final output.

For the purpose of these requirements, we consider data to be reproducible, if:
The products in a given event record are bitwise identical for integer fields
Floating point numbers in a given event record are the same to within the floating point precision of the machine they were generated on.
The ordering of data products within an event record are the same.
These criteria for reproducibility are designed to allow \dword{dune} to simulate “rare” events with well defined single event sensitivities, and to perform numeric transform operations on data values across different hardware platforms and runtime concurrency topologies.
\begin{itemize}

%\item It is also highly desirable that the framework broker access to random number generators and seeds in order to guarantee reproducibility.  Random number generation is an important aspect of code and given the additional complications of multi-threading and co-processors:

%16
\item It is highly desirable that the framework broker access to random number generators and seeds in order to guarantee reproducibility.  

%All of the preceding considerations imply that {\bf the software framework will need a very robust configuration system} capable of handling the requirements in a consistent, coherent, and systematically reproducible way.

%14
\item The framework must provide a full provenance chain for any and all data products which must include enough information to reproduce identically every persistent data product.  By definition, the chain will also need to include sufficient information to reproduce the transient data passed between algorithm modules, even though the product is not persisted in the final output.  
\end{itemize}


%\subsubsection{Provenance} % Does it need a separate subsection


%The need for transient data products is driven by the large Trigger Record sizes that can be encountered in the DUNE data, and whose transformation may be required, but for storage considerations are not written out.

%The use of highly parallel and heterogeneous computing environments leads to additional provenance requirements regarding the execution ordering and computing architectures on which the algorithms were executed.  The need for this information arises because of the possibility of different computing architectures producing different results, and of accelerators and other computing offload mechanisms producing different results than serially executed or non-accelerated codes (i.e. GPU accelerated code producing different results than the same code executed on the host processor).  Therefore 

%\begin{itemize}
%15
%\item the framework must provide full provenance information and all of the metadata required to ensure reproducibility.  
%\end{itemize}

This will include the computing architecture, including any specialized hardware used, on which the application is run as well as the runtime environment, execution model and concurrency level that the application used.  It is likely that a full picture of the necessary metadata will also require information only known to the \dword{wfms}.  In such a complex environment, it is highly desirable that the framework provide support to allow the effect of configuration changes and computing environments to be easily understood.


%%%%%% END OF REPRODUCIBILITY STUFF

\subsubsection{Configuration} \label{subsec:fworks:config}

Configuration is distinct from initialization of the framework objects; configuration happens first.  Given that \dword{raii} is an important concept for multi-threaded design, it is best to have a fully configured framework instance in the initialization phase.  Given the abundance of variations that make up \dword{hep} workflows a robust and easily programmable configuration system is a foundational component of all modern frameworks.  Some use a strictly declarative language and some use a Turing complete language.  The former must be augmented with scripts that write the declarations in a Turing complete language (usually it is part of the \dword{wfms}) because it turns out that control flow is a requirement.  Given this, there is a requirement that:

\begin{itemize}
%3 - removed Turing complete
\item The framework should provide a configuration language as a foundational component so that it can ensure coherence of its configuration.
\end{itemize}

% This is also mentioned in the unique challenges
%\todo{May be better to move/duplicate the mention of input file(s) into the I/O section. Workflow Allocator will supply the next file to process on request, as a transfer URL, which the framework needs to be able to open, whether POSIX file path or an xroot URL. Like with SAM, the framework should be able to do the next file request - it's just a REST API, using a JSON document supplied by Workflow to the job.}
As discussed in section \ref{sec:fworks:urgent}, the \dword{wfms} needs to be able to supply framework configuration parameters such as input file(s) or random number seeds to each framework application instance, which it should do using the framework’s configuration language.  To minimize errors, these parameters should be self-describing and validatable, and so:

\begin{itemize}
%4
\item the framework should provide a suitable API so that algorithm writers can ensure their required parameters are self-describing and validatable.

\item The framework should provide the concept of parameter sets that are nestable.  The set of all parameter sets that define a framework application instance should be identifiable, referred to here as a FrameworkConfigID.  Tracking that identity is one of the ingredients necessary to ensure scientific reproducibility.  However some parameters do not (and should not) change the algorithmic results, such as a debug print flag.  Independent of the state of such a flag it should be possible to define equivalence between FrameworkConfigIDs.

%5
\item The framework configuration system needs to have a robust persistency and versioning system that makes it easy to document and reproduce previous results.  It must be possible to create, tag, check-sum, store and compare configurations.  This configuration management system should be external to the framework or data files so that configurations can reliably be reused and audited.  

%Further fundamental requirements related to the framework configuration to guarantee reproducibility are:

%6
\item The resulting state of the configured framework and its components should be deterministic and reproducible given a set of environmental conditions which include the available hardware, operating system, input data, etc.  
\end{itemize}

%7 repeats the last point
%\item the ensemble of the framework+environmental conditions must give reproducible results.  

%Following on from the discussion of supporting fast analysis R\&D:

Related to the discussion on rapid turn-around in section \ref{subsec:fworks:modular}, but also important central features of the configuration system:

\begin{itemize}
%8
\item it is desirable that it should be possible to only configure those framework components required for a particular data processing use case.  

%This generates a further requirement that 

%9
\item it must be possible to derive the input data requirements for any algorithm, in order to define the sequence of all algorithms needed for a particular use case.
\end{itemize}
 
\subsubsection{Conditions Configuration}

In addition to the Trigger Record data, data processing requires access to other data from various sources, for example slow controls, detector status, and beam component status.  Such data is referred to generically as \dword{condmeta}, introduced in section \ref{subsec:db:conditions_metadata}, and also includes e.g. detector calibrations and any data external to the Trigger Record.  The time granularity or “interval of validity” of this data varies by source and is typically of much coarser granularity than individual Trigger Records, e.g. calibrations may be valid for months of data taking.  Meanwhile there are often several versions of \dword{condmeta} and correlations between \dword{condmeta} is not uncommon, making the coherent management of \dword{condmeta} a challenge in itself.  For this reason, \dword{condmeta} management should be external to the framework.

% Valid question but that discussion belongs in the Databases section.  For read cases you could indeed dump into a cvmfs directory and have the top-level folder be your configuration parameter.  For writing you want to have atomic DB transactions to support potentially complex calibration workflows.
%\todo{justify why this can't be done with cvmfs?}
Access to the external conditions data should preferably proceed via REST interfaces that support loose coupling of the framework and the external \dword{condmeta} management system.  As \dword{condmeta} may need to be transformed from its persistent format into a format required by an algorithm, and as multi-threading makes the cache validity of \dword{condmeta} complicated: 

\begin{itemize}
%18
\item the framework must provide a thread-safe conditions service that is a single point of access to \dword{condmeta}.  

%19
\item The configuration of the framework conditions service should ideally be via one configuration parameter (a global tag). 
\end{itemize}

Developers must not hard-code conditions data in their algorithms, although 

\begin{itemize}
%20
\item it must be possible to override a subset of global tag configured conditions for testing purposes.  
\end{itemize}

Developers usually find it convenient if such alternative conditions payloads can be provided outside of the main managed conditions system, e.g. via a local file.

\subsubsection{Concurrency} % Need to distinguish production and analysis
%Many aspects of DUNE data processing are well suited to concurrency solutions and {\bf the framework should be able to offload work to different compute architectures efficiently}, facilitate access to co-processors for developers and {\bf schedule thread-safe and non-thread-safe work}.  {\bf It must be possible to derive the input data requirements for any algorithm in order to define the sequence of all algorithms needed for a particular use case, and it must be possible to only configure those components required for that use case.}

The arrival of concurrency and heterogeneous architectures has added a further level of complication for framework designers and developers alike.  Much of the existing code-base still relies heavily on serial programming for CPU architectures, meanwhile the co-processor market is evolving rapidly resulting in a diverse hardware landscape.  Both multi-threading and co-processors present challenges for both frameworks and developers.  
\begin{itemize}
%10
\item It is therefore desirable that the framework should help facilitate the use of multi-threaded data processing and facilitate access to co-processors in an efficient manner. 
\end{itemize}


Multi-threading presents additional, well-documented challenges, particularly given that many important libraries are not thread-safe.  Summarizing briefly: algorithmic code, including sub-algorithms, should be thread-safe and must declare their compatibility to the framework.

\begin{itemize}

%13
\item The framework must be able to schedule thread-safe and non-thread-safe work appropriately.  
\end{itemize}

Thread-safety implicitly includes a general requirement on developers that algorithms and their sub-algorithms do not store state information in a thread-unsafe manner.  Further, data exchange should be done in controlled ways to ensure thread-safety, e.g. via some service which manages transient data - again the implementation of this challenging aspect is left to the framework designers.



\subsubsection{Externals including Machine Learning}
Machine learning is already heavily used in analysis of \dword{protodune} data and the framework should give special attention to machine learning inference in the design, both to allow simple exchanges of inference backends and to record the provenance of those backends and all necessary versioning information.  Finally, the framework should be able to work with both \dword{nd} and \dword{fd} data on an equal footing, and within the same job.

\dword{dune} will use several libraries outside of the framework software stack, and it will also be necessary to record the precise versions of all of these libraries to guarantee reproducibility of the physics results.  It is noted that containerization could potentially make provenance tracking easier in this respect.  

%% ANDREW Added
%(16a) The framework must provide deterministic results when run with different levels of concurrency.
%% and
%(16b) The framework must persist data in the same ordering when run under differing levels of concurrency.


One important source of external libraries relates to machine learning, and machine learning inference is expected to play a significant role in all stages of data processing including analysis.  

\begin{itemize}
%17
\item The framework should give special attention to machine learning inference in the design, both to allow simple exchanges of inference backends and to record the provenance of those backends and all necessary versioning information.


%Now move on to discussing "the" framework requirements, separate section? 

\end{itemize}


\subsection{Analysis\hideme{Norman/Laycock - still needs more}}

Based on the recommendations of the \dword{hsf} panel experts, the analysis use case is not required to be supported by the same software framework as that used for production.


Analysis includes the extraction of oscillation parameters, but is not limited to that, encompassing, as a minimum, comparisons between data and Monte Carlo, extraction of calibration and detector performance parameters, cross-section measurements, measurements of atmospheric, solar, and supernova neutrinos, and searches for non-standard phenomena.

The event-by-event paradigm is not a good match here. Spectra are filled by looping over neutrino candidates, or any other particle candidate, but then may undergo substantial processing in their own right. For example, the main work of an oscillation fit is the evaluation of many different combinations of oscillation and nuisance parameters.  While slices provide sub Trigger Record control flow, particle candidate control flow ignores the Trigger Record structure entirely.

% #33 - UNIQUE
%\item It must be possible to define arbitrary units of execution that are independent of Trigger Records.  It must be possible to correlate these units to Trigger Records for exposure accounting, and experiment conditions.


%34 - UNIQUE
%\item The framework should make minimal assumptions about the data model, i.e. it should not assume an event-by-event paradigm.
%%---

%35 - UNIQUE
%\item Analysis must be able to use particle-candidate-based control flow, without any constraints arising from the event-by-event paradigm.


Analysis work will be undertaken by a large number of collaborators, with varying levels of experience. In many cases, rapid feedback and iteration will be required to make progress:

\begin{itemize}
% added, changed #38
\item The analysis framework should have a low entry-level in terms of software expertise.
\end{itemize}

Experience shows that oscillation fits accounting for large numbers of systematic uncertainties are resource intensive, while analysts will likely only have access to modest local resources for prototyping and development.  Therefore the framework should make scaling and concurrency transparent both to the analyst and the developer as far as possible.  The use of declarative analysis techniques should be strongly encouraged to support this even when co-processors (and low-level implementation) changes.

\begin{itemize}
% adapted 39
\item The analysis framework should easily scale from local resources such as a laptop, up to multi-node compute at an \dword{hpc}.
\end{itemize}

It is noted that MPI-enabled frameworks written in python already exist and would be a good match to the above requirements.

Analysis files, of course, are derived from the data-processing framework files, and it must be possible to reconstruct this history. Due to the very large number of Trigger Records expected to be summarized in a single analysis file, the size requirements, and the fact that per Trigger Record information remains available in the parent files, we require:

\begin{itemize}

%40
\item Analysis files must record their parent framework files, but no Trigger Record provenance is required.  The full provenance information need not be retained in analysis files as this could easily become larger than the data itself.
\end{itemize}

One common and insidious class of mistakes is errors in exposure accounting and normalization. This is also a problem that is entirely solvable at the technical level.  Each individual Trigger Record (beam spill or other trigger) has exposure associated with it, whether POT or livetime or both. When filling a summary histogram from processing Trigger Records, the exposure should be calculated and stored as an integral part of the histogram, and operations between histograms should take correct notice of the exposure, e.g. ratio of one large exposure sample to a smaller exposure sample should produce a dimensionless ratio that has allowed for the differing exposures.

\begin{itemize}

%41
\item The framework must have native support for exposure accounting (POT and livetime), so as to make errors of this sort difficult.
\end{itemize}


All but the simplest analyses require a treatment of systematic uncertainties. There are three main technical means by which these systematics can be introduced. The most common, and most convenient, is reweighting. For example, the effect of various cross-section and flux uncertainties may be encapsulated by applying weights to Trigger Records of certain categories, to increase or decrease their representation in the final spectra. Secondly, Trigger Record data may be shifted. For example, an energy scale uncertainty may be most conveniently represented by rewriting of Trigger Records to increase or decrease reconstructed energies by a certain amount. Finally, the least convenient method is alternate simulation samples. The profusion of files requiring processing and bookkeeping makes this a heavyweight option, but in the case of uncertainties early in the analysis chain with complex effects, it may be the only way to handle them accurately. The treatment of systematics is cross-cutting across all analyses, it is important it is handled correctly, and the framework is able to offer substantive technical assistance.

In addition to being able to handle multiple input data streams:

\begin{itemize}
%42
\item The analysis framework should provide some means of cross-referencing (labelling) multiple input streams to correlate them in order to facilitate evaluation of systematic uncertainties.
\end{itemize}

For oscillation analysis, it will be important to work with both \dword{nd} and \dword{fd} data. Whether in an explicit joint fit, or where extracting constraints from the \dword{nd} to apply to the \dword{fd} analysis, there must be a uniformity in the treatment of various systematics. In general, experience gained with the \dword{nd} (where the majority of analysis work is likely to happen) should be transferable to the \dword{fd}.  This re-emphasizes the importance of the framework making minimal assumptions about the Data Model.

\begin{itemize}
%43
\item The analysis framework should be able to work with both ND and FD data on an equal footing, and within the same job.
%%---


\end{itemize}



\section{Timeline}

As the production use case for \dword{protodune} can be taken care of by the existing software framework, the development timeline is dictated by the needs of \dword{dune}.  Given that technology is rapidly evolving then the strategy is to address most of the development needs as late as possible. As the algorithmic code will be extant and developed against \dword{protodune} data, this late surge development effort will consist of 6 FTE years of core framework development effort, and 10 FTE years of effort to migrate the algorithmic code.  We assume the algorithmic migration effort will come from the wider collaboration, with a modest amount of support from core framework experts.
% Can we cite CMS here?
Assuming a small team of two core framework developers
%avoid single point of failure!
and a one year period for the migration, framework development effort should ideally start 4 years before the framework is needed for commissioning.
\todo{PJL 3/13 How much time and effort do we need?}

The exception to the above development strategy is the R\&D effort needed to accommodate the dynamic time window needs of the full \dword{dune} physics program, detailed in section \ref{sec:fworks:urgent}.  Given that this as-yet unknown solution is both central and critical to the success of the framework, development effort is needed now in order to minimize risk.
\dword{fnal} has recently approved a Lab Directors Research and Development project to start the process of designing an improved framework capable of satisfying many of these requirements. 

\todo{HMS 3/13 Check what I said about LDRD}

%\todo{I give up at this point ignore rest of chapter}
% {\hideme
% \subsection{Missing functionality \hideme{Norman/Laycock - needs more}}

% The DUNE production framework will ultimately need to handle much more demanding trigger record sizes than ProtoDUNE and this will be the main development challenge.  In addition, major development work will be needed for:

% \begin{itemize}
%     \item one
%     \item two
% \end{itemize}

% For analysis, early investigations into an MPI-based framework (CITATION NEEDED) already showed promising results.  The compatibility of DUNE analysis to HPCs suggests that this would be an efficient way for analysis computing resources to be provisioned to DUNE.

% \subsection{Plan to go forward \hideme{ Norman/Laycock -needed }}

% Development plan.
% }
%This plan was further reviewed by the HSF, maybe.
%
%\subsection{HSF recommendation for a path forward}
\end{document}