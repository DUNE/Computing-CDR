\documentclass[../main-v1.tex]{subfiles}
\begin{document}
\chapter{Code Management \hideme{Tom Junk, Mathew Muether, David Adams - draft}}
\label{ch:codemgmt}

\fixme{Doug: ND-LAr TMS and ND-GAr code management sections need to be written, references for CVMFS and Spack need to be found (anne 3/8: dword for cvmfs, spack footnote in prev chap)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Liquid Argon TPC Code Management \hideme{Junk/Calcutt - needs update}}
\label{sec:codemgmt:dunetpc}  %% fix label according to section

Software for simulation, data read-in, and reconstruction shares significant commonality across the %anne liquid-argon time projection chamber detectors 
DUNE \dwords{lartpc}. %anne planned to be deployed by DUNE and also  prototypes.  
These comprise the %anne horizontal- and vertical-drift far detector modules, the 35-ton prototype, ProtoDUNE-SP, ProtoDUNE-DP, ProtoDUNE-2, ProtoDUNE-VD, ICEBERG, 
the \dword{sphd} and \dword{spvd} \dwords{detmodule}, the \dword{35t}, \dword{pdsp}, \dword{pddp}, \dword{protodune2},  the pixel \dword{nd} and its prototypes.  The same software stack handles the wire readout, the photon detectors, and the cosmic-ray taggers used in some of the prototypes.  The interfaces to event generators, GEANT4, {\it art} and the data handling systems require effort to build and maintain.  Data products need to be thought out carefully.  To reduce the development burden and expand the pool of expertise, DUNE has chosen to use the LArSoft toolkit, which is built on NuTools and {\it art}.  

LArSoft itself is split up into several repositories, such as larcore, larsim, larevt, lardata, and variations that have been split from them over time, such as lardataobj.  LArSoft is supported by Fermilab's Scientific Computing Division and is shared among several other experiments, such as ArgoNeuT, MicroBooNE, ICARUS and SBND.  The pool of developers is quite large, and the rate at which new code is added is quite significant.  New releases of LArSoft are made weekly.  

DUNE's LArSoft-based software historically had been collected in a single git repository called dunetpc, starting in the early days of the collaboration.  This repository was hosted using Fermilab's Redmine service.  Compiling all of the code in this single repository became slower as more source files were added.  In January 2022, a split of the dunetpc repository into ten smaller repositories was deployed on GitHub, taking advantage of GitHub's superior performance, feature set, and openness compared with Redmine.  The old dunetpc repository is kept in a read-only state in Redmine for inspection purposes.  The new top-level product is called dunesw.  A UPS dependency graph made in January 2022 is shown in Figure~\ref{fig:dunetpcdeptree}, showing the dunesw stack, the LArSoft UPS products, and their dependencies.

\begin{dunefigure}
[Dependency graph for the dunesw software stack, Jan 2022]
{fig:dunetpcdeptree}
{UPS dependency graph for the dunesw software stack, for version v09\_42\_00\_01, January 2022.}
\includegraphics[width=\textwidth]{graphics/CodeManagementFigures/dunesw_v09_42_00_01_graph.pdf}
\end{dunefigure}

Since the dunesw stack depends on LArSoft and sets it up via UPS, dunesw also has a weekly release schedule, and releases are tied to LArSoft's releases and use the same version tag names.  LArSoft's repositories had been hosted in Fermilab's Redmine instance until 2018, at which point they were migrated to GitHub.  At the same time, a pull-request model was instituted.  LArSoft's pull-request system, based on the one used by CMS, performs automated checks that the proposed new code compiles and tests properly.  Once the tests are successful, a review of the proposed new code is required before it can be merged into the repository.  As of this writing, DUNE is considering moving to a pull-request model similar to that of LArSoft.

The build system used is {\tt mrb}, which is provided by and supported by Fermilab's SCD.  Software is built in UPS products which are distributed to the collaboration in two ways.  The primary distribution mechanism is via a set of installed products in CVMFS which can be directly set up by end users without the need to install anything locally.  CVMFS maintains a cache on the user's computer to store copies of the released software files for repeated local access.  CVMFS is also used to distribute released software to batch worker nodes.  The second software distribution mechanism is via the scisoft.fnal.gov web server.  When a release is made, the repositories are tagged and the release manager triggers Fermilab's Jenkins build servers to compile the software and install auxiliary files in UPS products.  The UPS products are then tarred up and the tarballs are uploaded to the scisoft web server, along with a manifest file which describes which tarballs need to be downloaded and installed to get a full, runnable dunesw software stack.

In addition to code, some data files also are distributed via CVMFS.  For example, photon lookup libraries are stored in several-hundred-megabyte rootfiles which are inconvenient to store in a git repository.  They are loaded into a special repository in CVMFS and scisoft called dune\_pardata.  Even larger files are stored in StashCache which has a CVMFS interface for the user, but which retrieves files out of a dedicated area in dCache.

We are evaluating Spack as a replacement for {\tt mrb} and UPS.  Spack is a build system that also allows users to select from a list of installed versions, like UPS.  As operating systems become more secure, some of the ways of distributing, setting up, and running HEP software have conflicted with security features of the operating systems.  One of these is the use of the symbol {\tt LD\_LIBRARY\_PATH}.  UPS uses {\tt LD\_LIBRARY\_PATH} to point a user's environment at directories that contain the versions of released products that can be loaded and run.  Changing the contents of the {\tt LD\_LIBRARY\_PATH} variable causes the same program to load different versions of libraries.  Proper use of UPS maintains compatibility.  However, with newer operating systems, the shared libraries that are linked with programs and other shared libraries must be in the same locations as they where when linked, and not relocated.  The library that links to another keeps track of the installed locations of the libraries it links with.  Alternatively, the paths in which the dependent libraries are found can be edited after installation, in effect re-performing a stage of the linking step.  Spack is aware of this and inserts the proper {\tt rpath} values into linked libraries.

The use of Spack is under evaluation.  UPS is now approximately 25 years old and it is not an industry standard, and it is somewhat linked to Fermilab.  Spack has a significant community using it outside of HEP, and thus there is a large volume of documentation available for it on the web.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Near Detector Code Management \hideme{Muether/Cremonisi/Junk needs update}}
\label{sec:codemgmt:neardet}  %% fix label according to section

The near detector consists of three components.  When DUNE starts running with beam, the expected near detector configuration will be a liquid argon TPC with pixel readout (ND-LAr), a magnetized steel muon spectrometer (TMS), and an on-axis beam monitor, SAND.  A future upgrade is proposed to replace TMS with a gaseous argon TPC with pixel readout, an ECAL and a muon system (ND-GAr). The software structure reflects the detector configuration.  The software effort for each detector has contributions from the groups that are designing and proposing to construct the detectors.  Below is a summary of the code management strategies used for each subsystem, and also for the shared tools.

\subsection{ND-LAr Code Management}
\label{sec:codemgmt:ndlar}

\subsection{TMS Code Management}
\label{sec:codemgmt:tms}

\subsection{ND-GAr Code Management}
\label{sec:codemgmt:ndgar}

The software for simulating and reconstructing events in ND-GAr is, at the time of writing, contained in two repositories:  {\tt garsoft} and {\tt garana}.  GArSoft builds on the functionality of {\it art} and NuTools, and it is maintained when the {\tt art} API changes and when the build system is upgraded.  Following LArSoft's pattern, it is built with {\tt mrb} and set up with {\tt UPS}.   A description of the functionality of GArSoft is in Secs.~\ref{sec:usecases_ndgardetsim} and~\ref{sec:algo:reco:gartpc:pixels}.  GArAna provides a facility for making an analysis ntuple from information stored in GArSoft data products.  Both repositories are hosted at GitHub.  Continuous integration has not yet been set up for GArSoft and GArAna.  Executable binaries and the associated libraries are built using Fermilab's Jenkins build servers, and the build artifacts are installed in CVMFS so that the software can easily be run on the grid.

\begin{dunefigure}
[Dependency graph for the GArSoft software stack. March 2022]
{fig:garsoftdeptree}
{Dependency graph for the GArSoft software stack, for version v02\_16\_00, current as of March 2022.}
\includegraphics[width=\textwidth]{graphics/CodeManagementFigures/garsoft_v02_16_00_graph.pdf}
\end{dunefigure}

\subsection{SAND Code Management}
\label{sec:codemgmt:sand}

\subsection{Near Detector Production Tools}
\label{sec:codemgmt:ndproduction}

A repository has been set up in GitHub to manage files that are needed for near detector production.  These include production scripts, detector geometry descriptions and flux specifications.  It is versioned and installed in CVMFS like other DUNE repositories, so that production workflows can be reproduced at later dates.

\section{Continuous Integration \hideme{Junk - draft}}
\label{sec:codemgmt:ci}

The stability of the performance of a large base of shared software requires attention to each change that is committed.  Software releases require validation and approval before being used in analyses intended for publication.  The rapid pace of software development early in the life cycle of the experiment, as well as increased activity as data are first collected and conferences come up, requires constant vigilance to ensure that bugs are not introduced and that software remains backwards compatible.

To meet this requirement, an automated continuous integration (CI) system is currently in operation for the LArSoft-based code base.  Similar systems, even using the same infrastructure, will be deployed for the near detector and beam simulations system.  The CI system consists of a set of servers that monitor commits pushed to the central code repositories.  On each commit, with a suitable delay of approximately 15 minutes in order to aggregate commits.  The CI system can also be triggered by user interaction, via an authenticated request.

Once triggered, the CI system will compile the changed code as well as any dependencies that are required.  Currently, the CI system builds LArSoft and all experiment code from the head of the develop branch in the repositories.  This is done because new commits to one repository, be it experiment-specific code or shared LArSoft code, may depend on the latest version of software in other repositories.  If shared software is the subject of the commit, then software that depends on it, such as all the experiment software stacks, are rebuilt, as a change to a header file in a shared repository can cause any source file that includes it, even indirectly, to fail to build.

The status of the build is stored in a logfile and summarized on a web page.  If a commit causes the build to fail, software managers and the person who committed and/or pushed the commit are notified and the commit is blocked from being merged into the head of develop.  LArSoft currently implements a pull-request model, in which experiment-appointed Level-2 managers comment on and sign off on changes to the central code, and Level-1 managers perform the actual merging.  A proposed change will not even be sent out for approval unless the CI system can build it and validation tests are run to compare output that ought not to have been changed by the new code.

This second step, physics performance validation, requires a more lengthy run through both unit tests and integration tests that run simulation and reconstruction workflows.  These tests run on standard input files and have their random number seeds fixed to constants.  The outputs are compared with reference histograms and a web page summarizes the comparison of the output logfiles, histograms, as well as run times and memory consumption, all of which are available on a web site that monitors these tests.  History plots of variables such as run time and memory consumption are made available on the monitoring web site so that investigations of when the memory usage of a job jumped up can be done without laborously checking out, building, and running the software in the suspected timeframe looking for a particular change.
\end{document}
