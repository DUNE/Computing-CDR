\documentclass[../main-v1.tex]{subfiles}
\begin{document}
\chapter{Networking \hideme{Mike Kirby and Peter Clarke - in progress}}
\label{ch:netw}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{xyz}
%\label{sec:netw:xyz}  %% fix label accordinean to section


\todo{{\it }Kirby to write first about US side of the network including 
- US OSG site connectivity 
- FNAL connection to \dword{esnet}
Pete then hooks Europe on to this.}

With the DUNE experiment located at two geographically distant sites and having computing resources distributed around the globe, the design and operation of networking will be very important to the success of the DUNE experiment. The network requirements can in general divided between three may areas of concern: Far Detector, Near Detector, and distributed computing. Assuring that interconnection and bandwidth requirements between each are met with appropriate up time is important for operations and successful physics results. The requirements for each system are based upon current estimates of data rates for both the FD and ND DAQ, technical networks, and slow control networks.

The Far Detector networking encompasses the local area network at SURF, the connection to the wide area network, and the network path back to storage services at Fermilab in Batavia, IL. This network path between the Far Detector at SURF and the Fermilab campus is the most important network path for the DUNE experiment. This path will be used for  transfers of raw data, and also for connectivity between SURF and operations centers at Fermilab and elsewhere. While operations traffic is expected to use this connection, it should be noted that safety systems will not rely upon the network connection.

The requirements for the network connection between SURF and Fermilab were explored extensively in the High Energy Physics Network Requirements Review\cite{bib:osti_1804717} . The primary bandwidth requirement is determined from both the steady-state yearly transfers from the DAQ, and by burst transfer rates during extended-readout trigger records. The Consoritum Interface document between the DUNE DAQ Consortium and the DUNE Computing Consortium states that no more than 30 Petabytes (PB) of data will be output from the DAQ for permanent storage. This translate into 7.98 Gigabits per second (Gb/s) of steady-state data transfer from SURF to FNAL, and includes beam triggered, cosmic triggered, and calibration raw data. The most extreme burst requirement considered in the ESNet review was the situation of processing an extended-time trigger record for a SuperNova candidate. These, in contrast to the steady beam and cosmic-ray rate,
are expected to occur one to two times/month and involve data volumes of up to 200 TB of compressed far detector data. The ability to produce a reasonably fast pointing signal would be extremely valuable to optical astronomers doing follow-up, especially if a supernova was in a region where dust masks the primary optical signal. The need to be alert to supernovae and to quickly transfer and process these data imposes significant requirements on triggering, data transfer, and reconstruction beyond those imposed by the more regular beam-based oscillation physics. For example, a compressed supernova readout of all four modules will be on the order of 184 TB in size and take a minimum of four hrs to transfer over a 100 Gb/s network, and then take on the order of 130,000 CPU-hrs for signal processing at present speeds. If processing takes the same time as transfer, a peak of 30,000 cores would be needed. In order to have initial analysis results of a SN trigger record within one day, the network between the Far Detector at SURF and the Fermilab campus is being designed for a capacity of 100 Gb/s.

As the network will be used for both raw data transfer and for remote operations, there is a requirement to have both a primary and secondary network path between the Far Detector and Fermilab. Working in conjuction with ESNet, the Core Computing Division at Fermilab has development a deployment plan for two geographically separate network paths. The map of the proposed paths is shown in Figure \ref{fig:esnet_network_path_map} with the two paths merging between St. Cloud, MN and the Fermilab campus. The primary path connects network switches at the top of the Ross Shaft to Fermilab via ESNet infrastructure. As of Feb 2022, the primary path is available from Lead, SD to Fermilab at limited bandwidth of 10 Gb/s. In order to complete this path, a vendor will be secured to provide infrastructure between Lead and the Ross Shaft. After completion, the primary path will provide 100 Gb/s guaranteed bandwidth. The secondary path will provide 10 Gb/s of network bandwidth and should be available in CY22 or CY23 and will be from the Yates shaft to Fermilab via ESNet. The networking is expected to be completed in stages with both paths capable of the full design bandwidth by the time that Far Detector physics operations commences. A detailed schedule for networking is shown in Figure \ref{fig:esnet_network_timeline}. Additionally, a tertiary path that utilizes vLAN infrastructure from the South Dakota Higher Education Network (REED) and a southern path through Colorado and Kansas City, KS will provide 10 Gb/s of bandwidth. During construction, the tertiary path will serve as the primary link to the DUNE FD and then revert to a tertiary role once the dedicated primary path is complete.

The network uptime is defined by the capability of the Far Detector DAQ system to stage raw data locally in case all connectivity is lost. The FD DAQ is designed to have capacity to store one week of raw data from the detector on local storage at SURF. Were connectivity to be lost for an entire week and given the nominal rate of 7.98 Gb/s from the DAQ, assuming that the NIC present in the local storage have infinite bandwidth, the additional 92 Gb/s of bandwidth available on the primary path means that the backlog in raw data would be cleared within one day of reconnection. And given the presence of three geographically separate paths from SURF to Fermilab, if all three networks have an uptime of > 90\%, then the expected networking live time (99.9\%) is more performant than the DAQ required live time (98\%) by more than a factor of 10.

\begin{dunefigure}
[ESNet map of network path]
{fig:esnet_network_path_map} 
{A map of the geographically separate networking paths proposed for primary (blue) and secondary (red) network paths between SURF and Fermilab.}
\includegraphics[width=0.9\columnwidth]{graphics/Networking/DUNE_ESNet_network_path.png}
\end{dunefigure}

\begin{dunefigure}
[Networking Timeline]
{fig:esnet_network_timeline} 
{The current timeline for the implementation of networking connection between SURF and Fermilab.}
\includegraphics[width=0.9\columnwidth]{graphics/Networking/networking_timeline.png}
\end{dunefigure}

 q
%\todo{Some of this may move into cooperation section}

Connection to European sites is accomplished via \dword{esnet} peering with \dword{geantnet}.  
\dword{esnet} provides redundant transatlantic links at xxxx Gbit/s. These peer with \dword{geantnet} in ???London??? and ???Amsterdam??.
\dword{geantnet} is the EU wide research network with a core capacity of several hundred Gbits/s. \dword{geantnet} in turn peers with \dword{nren}s  in each participating country, although details pertaining to each DUNE site vary. As example, in the UK the \dword{nren} is JISC-JANET, which at the time of writing has a 400 Gbit/s core and connects to the GridPP-RAL site redundantly at 100 Gbits/s. 
Similarly CC-IN2P3 site in France is connected to RENATER at 100 Gbits/s, and the FZU site in the Czech Republic is connected to CESNET at 100 Gbit/s Gbit/s.  DUNE expects all participating countries to ensure that as part of their pledges of CPU and storage capacity, that the sites offered have commensurate network connections. We do not expect any systematic issues to arise, and none have done so in the WLCG/LHC context. DUNE participates in the worldwide HEP Network coordination body.

 Layer-3 \dword{vrf} provision is now very prevalent in HEP. \dword{vrf}s provide a logical routing overlay that can allow for traffic engineering to utilise high capacity paths where needed. The LHC community uses a \dword{vrf} called LHCONE, and this has also been used for DUNE traffic along with other non-LHC experiments such as Belle II. 
At present DUNE is agnostic to the use of LHCONE, and since FNAL is connected to LHCONE it can easily accommodate sites with or without such provision.
Investigations are currently underway to determine the technical requirements for the creation of a separate DUNEONE \dword{vrf} were it to ever be required. It is however not currently foreseen, and does not form part of our baseline planning.
\end{document}