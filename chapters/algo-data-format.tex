\documentclass[../main-v1.tex]{subfiles}
\begin{document}
\chapter{Data Formats}
\label{ch:format}

\section{Introduction \hideme{Schellman/Norman - draft}} 

\dword{dune} data are stored and processed in a variety of forms and representations which facilitate and are tuned to the analysis that is perform on them.   In general organization, the data are characterized and classified by their \dword{datatier} and their data format within that tier.  The data tier corresponds to the stage of the data in its lifecycle and its progress from the original detector acquisition data and simulations, towards its final analysis.  The data format refers to the low level representation and organization of the data format that it takes on in either a persistant or transient form during the analysis workflows.  This classification of data by tier and format allows DUNE to develop, catalog, and maintain its data ecosystem in the context of the larger computing model and computing resources.


\section{Data tiers \hideme{Schellman/Norman }}

As the data from the DUNE detectors and simulation systems are processed, they evolve through a series of \dwords{datatier}.  The \dword{datatier} concept was first adopted by the D0 and CDF collaborations two decades ago and is an important component in the definition of datasets in the data catalog.   Examples of common \dwords{datatier} enumerations are ``raw'' for raw detector data, ``hit-reconstructed'' for data that has had initial reconstruction of regions interest and activity performed upon it, ``full-reconstructed'' which has more advanced reconstruction to identify higher level physics objects and interaction regions, and many others representing the information that is present and the algorithms that were used to produce it.

The algorithms which are run against data, transform the datafiles from one \dword{datatier} to another in what is effectively a directed graph.  This allows deterministic workflows to be constructed in a module fashion and for dependancies in the data representation to be identified.  This also allows for different branches and paths to be taken logically in the data lifecycles and for the overall computing model to manage these transforms that advance the state of the data in much the same manner as one would expect from a simple state machine.

Most importantly, this concept of data tiers allows for different information and informational representations to be present in each tier.  This allows for easier management and storage of data, (as down selects can be applied at different tiers to reduce overall size footprints) while still permitting specialized applications to be developed that require specific data products or representations be present in the data streams.  This approach also allows for both common and parallel treatments of simulation and real detector data, which is key to the neutrino analysis process. 

\begin{dunetable}
[Data Tier Summary]
{l l r c l l} 
 {table:format1}
 {Example data tiers.  The sizes given are the total on tape, not the amount present on disk under the retention policies.}
Data Tier & Produced by &current size & Copies & Disk Lifetime  & Tape Lifetime \\ [0.5ex] 
raw&DAQ&5.2 PB&2&few months&$>20$years\\
detector-simulated&geant4+detsim&4.1 PB&1&transient&5-10 years\\
hit-reconstructed&hit finding&0.2 PB&1&transient&5-10 years\\
full-reconstructed (MC)&pattern recognition&5.7 PB&1&2 years&5-10 years\\
full-reconstructed (data)&pattern recognition&3.4 PB&1&2 years&5-10 years\\
root-tuple&data reductions&24 TB&1
&2-3 years&5-10 years\\
\end{dunetable}

This diversity in tiers can be seen by examining the projected estimates for the DUNE data and retention policies for each tier.  Table \ref{table:format1} summarizes some of the more common formats for DUNE, along with their sizes and expected retention. 

The DUNE computing model is predicated on this type of an organizational structure, and it is used throughout the data management, computing, workflow organization and data placement models that are discussed in this document.

%Within a data tier, data can have different formats.  An example is the use of \dword{root} formats for ProtoDUNE Run I raw data and \dword{hdf5} format for ProtoDUNE Run II.

\section{I/O formats \hideme{Schellman/Bashyal - draft}}

The low level representation of the DUNE data is keyed to the data tier(s) at which they exist and are accessed.  It is common for these representations to change over the data lifecycle to reflect organizational needs of the algorithms that are run on the data.  In particular in HEP it is common for raw data coming from the detector subsystems and DAQ systems to have representations that are closely tied to the electronics and readout (and is often proprietary due to optimizations that are needed for high speed readouts), while later stages of processing use more generic formats and representation are more amiable to use in algorithms and transforms.  In the HEP and neutrino community, there have been a number of different ``high level'' data representation and I/O systems that have been used.  Starting in the late 1990's the ROOT I/O system became one of dominate systems for HEP data, and has continue to feature prominently for experiments which use heavily C++ based software stacks.  More recently, the rise of machine learning and artificial intelligence software has lead to other I/O systems, which interact better with Python bases software stacks, to become more common in HEP analysis workflows, and the move towards more parallel data processing models has additionally lead to the development and shift towards data formats that allow for parallel data access.

In the case of DUNE, we have historically represented our data through a combination of proprietary and \dword{root} I/O based data formats.  In our initial ProtoDUNE run 1 data processing, and in our current simulation chains, this has been our strategy and is fully supported throughout our processing chain.  \dword{root} I/O provides data objects and methods that have been optimized for certain types of \dword{hep} applications and access methodologies.  The \dword{larsoft} framework, which is discussed in section~\ref{sec:framework:status}, is compatible with the ROOT I/O systems.

However, \dword{hep} increasingly uses   machine learning methods that rely on non-\dword{hep} data formats, notably the \dword{hdf5} format.  \dword{hdf5} is designed to save data as arrays of fundamental types, such as integers and floating-point numbers, along with metadata such as group names and dataset names.  \dword{root} files support a much more complex structure.  \dword{root} provides mechanisms for saving and retrieving data in C\raisebox{1pt}{++} classes, provides for automated conversion of data on reading when classes change from one version of user code to the next (schema evolution), and provides structures like trees. \dword{root} also supports integrated streaming through \dword{xrootd}. 

\dword{hdf5}'s uniformity of data representation has both advantages and disadvantages. For simple, repetitive data structures, such as the raw waveforms generated by \dword{dune}, the simplicity of \dword{hdf5} is useful.  Many third party machine learning algorithms use \dword{hdf5} and users are already converting \dword{root} to \dword{hdf5} when using these algorithms.  However, for interaction reconstruction, \dword{root}'s features for processing complex data objects optimized for \dword{hep} and integration with the \dword{art} framework are useful and convenient.  The current \dword{dune} processing chain uses both formats, and the choice has been optimized for each purpose.

In particular, the \dword{protoii} \dword{daq} system now writes raw data in \dword{hdf5} format. The utilization of \dword{hdf5} libraries within the \dword{daq} software allows for multiple processes to write to the same output file asynchronously. This ability is very advantageous when acquiring data from multiple \dword{fd} modules or multiple \dword{nd} subdetectors. The implementation of \dword{hdf5} is being tested %anne within  \dword{pdcoldbox} 
in the \dword{np02} \coldbox at the \dword{cern} with a small prototype of the %anne \dword{pdvd} 
\dword{spvd} \dword{protodune} readout systems. 
\todo{anne: \dword{pdcoldbox} and \dword{pdvd}  aren't standard per the \dword{spvd} CDR }

With the advent of the new \dword{daq} format, \dword{hdf5} formatted data have been integrated successfully with the \dword{art} and \dword{larsoft} frameworks in a manner that allows improved memory management for large data records.  The  input \dword{hdf5} record is parsed and pointers to large data arrays are then made available, sequentially, to the \dword{art}/\dword{root} event processing framework for further processing. Development work is needed to understand if implementation of streaming for \dword{hdf5} data over the network is possible or needed for efficient distributed processing of raw data files.


\subsection{Future \hideme{Laycock - check for redundancy}}
A quantitative review of data formats by Blomer {\it et al.}\cite{Blomer:2018icl} 
highlighted the importance of the data format and access libraries to analysis turn-around times.  The results demonstrated that the effort to optimize the \dword{root} I/O system has been an excellent investment as it out-performed all other data formats, including \dword{hdf5}, except for a few corner cases
\footnote{Pure serialization using Google's Protobuf library was seen to be faster as it does not manage columns.}.
In the near future, the next generation \dword{root} data format, called RNtuple 
\cite{Blomer:2020usr, ROOTTeam:2020jal} promises even more I/O optimization for \dword{hep} use cases.  The \dword{hsf} input to the high luminosity \dword{lhc} computing review \cite{HEPSoftwareFoundation:2020daq} stressed that, particularly for I/O bound analysis, it's unlikely that a drop-in replacement for \dword{root} will be found.
Indeed, the \dword{lhc} experiments are expected to migrate to the RNtuple format on the timescale of \dword{dune}.  Using the same data format as the \dword{lhc} experiments would have clear advantages for \dword{dune}, particularly with regard to enabling interoperability of various elements of the software stack. \dword{dune} plans to work together with the \dword{root} team to investigate potential solutions for the issues seen with \dword{protodune} data.

Despite the success of \dword{root}, there are other popular data formats like \dword{hdf5} that \dword{dune} will continue to investigate.  The astronomy community produces data not dissimilar to \dword{dune} and the Advanced Scientific Data Format (ASDF) addresses some of the limitations of \dword{hdf5} while retaining its convenience. Another key consideration of data formats is compatibility with the tools of the wider data science community, particularly for machine learning applications, and \dword{dune} will need to follow developments here too.  A common solution to this compatibility issue in \dword{hep} experiments is to use an interface layer that can mitigate the choice of persistent data format with some performance cost that may often be negligible in many use cases.  The scikit-hep project\cite{Rodrigues:2019nct} provides a python ecosystem to read \dword{root} persistent data formats and convert them to native data science formats.

Given the vast variation in scale and data access patterns that \dword{dune} computing needs to support, it is likely that most if not all of these options, and additional as-yet-unknown options, will have a role in creating the most effective suite of solutions to support the physics program. Detailed studies of the \dword{protodune} dataset will be needed to make cost-benefit analyses of the various options and guide the analysis model.  A final consideration, and by no means the least important, will be the ease of use of the analysis model for physicists to maximize their analysis output.

\end{document}