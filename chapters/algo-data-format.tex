\documentclass[../main-v1.tex]{subfiles}
\begin{document}
\chapter{Data Formats}
\label{ch:format}



\section{Introduction \hideme{ Anne's comment's addressed? }} 

\dword{dune} data are stored and processed in a variety of forms and representations which facilitate and are tuned to the analysis that is performed on them.
In general, %organization, 
the data are characterized and classified by their \dword{datatier} and their data format within that tier.  The data tier corresponds to the stage of the data in its lifecycle and its progress from the original detector acquisition data and simulations, towards its final analysis stage.  The data format refers to the low level representation and organization of the data format that it takes on in either a persistent or transient form during the analysis workflows.  This classification of data by tier and format allows \dword{dune} to develop, catalog, and maintain its data ecosystem in the context of the larger computing model and computing resources.


\section{Data tiers \hideme{Schellman/Norman }}

As the data from the \dword{dune} detectors and simulation systems are processed, they evolve through a series of \dwords{datatier}.  The \dword{datatier} concept was first adopted by the D0 and CDF collaborations two decades ago and is an important component in the definition of datasets in the data catalog.
The algorithms which are run against data, transform it from one \dword{datatier} to another in what is effectively a directed graph.  This allows deterministic workflows to be constructed in a modular fashion and for dependencies in the data representation to be identified.  This also allows for different branches and paths to be taken logically in the data lifecycle and for the overall computing model to manage these transforms that advance the state of the data in much the same manner as one would expect from a simple state machine.

Most importantly, this concept of data tiers allows for different information and informational representations to be present in each tier.  This allows for easier management and storage of data, (as down selections can be applied at different tiers to reduce overall storage footprints) while still permitting specialized applications to be developed that require specific data products or representations be present in the data. %streams. PJL
This approach also allows for both common and parallel treatments of simulation and real detector data, which is key to the analysis process. 

\begin{dunetable}
[Data Tier Summary]
{l l r c l l} 
 {table:format1}
 {Example data tiers.  The sizes given are the total on tape, not the amount present on disk under the retention policies.}
Data Tier & Produced by &current size & Copies & Disk Lifetime  & Tape Lifetime \\ [0.5ex] 
raw&DAQ&5.2 PB&2&few months&$>20$years\\
detector-simulated&geant4+detsim&4.1 PB&1&transient&5-10 years\\
hit-reconstructed&hit finding&0.2 PB&1&transient&5-10 years\\
full-reconstructed (MC)&pattern recognition&5.7 PB&1&2 years&5-10 years\\
full-reconstructed (data)&pattern recognition&3.4 PB&1&2 years&5-10 years\\
root-tuple&data reductions&24 TB&1
&2-3 years&5-10 years\\
\end{dunetable}

This diversity in tiers can be seen by examining the projected estimates for the \dword{dune} data and retention policies for each tier.  Table \ref{table:format1} summarizes some of the more common formats for \dword{dune}, along with their sizes and expected retention. Examples of common \dwords{datatier} enumerations are ``raw'' for raw detector data, ``hit-reconstructed'' for data that has had initial reconstruction of regions of interest
%and activity PJL, just doesn't scan
performed upon it, ``full-reconstructed'' which has more advanced reconstruction to identify higher level physics objects and interaction regions, and many others representing the information that is present and the algorithms that were used to produce it.

The \dword{dune} computing model is predicated on this type of organizational structure, and it is used throughout the data management, computing, workflow organization and data placement models that are discussed in this document.

%Within a data tier, data can have different formats.  An example is the use of \dword{root} formats for \dword{protodune} Run I raw data and \dword{hdf5} format for \dword{protodune} Run II.

\section{I/O formats \hideme{Schellman/Bashyal/Norman}}

The specific data format of each data tier can have a large impact on the performance of processing and accessing that data tier.
A review of data formats and their serialization performance with respect to LHCb data, was performed by Blomer based on the state of major data formats in 2017\cite{Blomer:2018icl}.  While the actual results have since been superseded by improvements in the formats he examined, his study
highlighted the importance of the data format and access libraries to analysis turn-around times in \dword{hep}.

The low level representation of the \dword{dune} data is keyed to the data tier(s) at which they exist and are accessed.  It is common for these representations to change over the data lifecycle to reflect organizational needs of the algorithms that are run on the data.  In particular in \dword{hep}, it is common for raw data coming from the detector subsystems and DAQ systems to have representations that are closely tied to the electronics and readout (and is often customized due to optimizations that are needed for high speed readouts), while later stages of processing use more generic formats and representations more amenable to use in algorithms and transforms.  In the \dword{hep} and neutrino community, there have been a number of different ``high level'' data representation and I/O systems that have been used.  Starting in the late 1990's the \dword{root} I/O system became one of the dominant systems for \dword{hep} data, and has continued to feature prominently for experiments which use heavily C++ based software stacks.  
% PJL This would be taken badly by the Root team, and the file format is often unchanged (access via scikit-hep)
%More recently, the rise of machine learning and artificial intelligence software has lead to other I/O systems, which interact better with Python bases software stacks, to become more common in \dword{hep} analysis workflows, and the move towards more parallel data processing models has additionally lead to the development and shift towards data formats that allow for parallel data access.

In the case of \dword{dune}, we have historically represented our data through a combination of custom and \dword{root} I/O based data formats.  In our initial \dword{protodune} run 1 data processing, and in our current simulation chains, this has been our strategy and is fully supported throughout our processing chain.  \dword{root} I/O provides data objects and methods that have been optimized for certain types of \dword{hep} applications and access methodologies.  The \dword{larsoft} framework, which is discussed in section~\ref{sec:framework:status}, is compatible with the \dword{root} I/O system and other data formats.

%However, n
Neutrino science, and \dword{hep} more generally, has been increasingly relying on machine learning methods for neutrino interaction classification, particle track reconstruction, optimized event selection and more.  The AI/ML toolkits, which have been developed by the computational sciences and by major computational industries, are used by \dword{dune} but do not use 
% Nowadays this is easily solved with scikit-hep
%nor are natively compatible with 
the \dword{root} I/O data formats and systems.
%(It should be noted that this is not unexpected, as \dword{root} I/O is a highly \dword{hep} domain specific technology that has not seen wide adoption outside of the high energy and nuclear physics communities) 
Instead these AI/ML toolkits, written in Python, are often designed around 
data formats from the data science community,
%more widely accepted formats and I/O systems, such as the Hierarch 
notably the \dword{hdf5} format.  The \dword{hdf5} format is very well supported across the Python programming language and in particular by the major scientific python libraries and frameworks.  

A common solution to the compatibility issue in \dword{hep} experiments is to use an interface layer that can mitigate the choice of persistent data format with some performance cost.  The scikit-hep project\cite{Rodrigues:2019nct} provides a python ecosystem to read \dword{root} persistent data formats and convert them to native data science formats.

The \dword{hdf5} format is designed to save data as organized tables of fundamental data types along with links between tables which then allow for complex record structures similar to modern databases.  The strength of \dword{hdf5} is that its tabular structure allows very
%for more  - implicit comparison to ROOT, to be avoided
efficient ``columnar'' analysis of data (i.e. analysis of one or more variables from across a dataset).
%ROOT works in branches, it supports columnar very well, but when we write code that 
% explicitly uses rows... it's not magic.  Hence RDataFrame.
%compared to the same type of operation performed on more traditional ``row-wise'' or event record based organizations which often incur greater overheads in the retrieval of information from across records.  
The format also has native support for parallel data access, and in particular for parallel reading and writing of compressed data.  This support extends to highly parallel file systems, such as those found in leadership computing facilities, and has been tuned to scale extremely well using the MPI protocol in these environments.  
For the large \dword{dune} event data, which can be highly compressed, this support for parallel I/O is advantageous, especially in the near realtime data acquisition environment.  As a result of these features, and the use of the format in other segments of \dword{dune}, we expect to support the \dword{hdf5} format as part of our computing model.

In contrast to the \dword{hdf5} format,
%which is highly supported in Python, % we got it already
the \dword{root} I/O format has support for the recording and retrieval of complex data structures in the  C\raisebox{1pt}{++} language, and can be used to serialize and deserialize  C\raisebox{1pt}{++} structures and classes.  It 
is naturally
%has also been highly 
integrated with the \dword{root} data analysis framework which, while domain specific to \dword{hep}, is nonetheless one of the largest and most robust toolkits for \dword{hep} data analysis.  We also expect that support for parallel I/O within the \dword{root} I/O systems will be expanded greatly by the time of the \dword{dune} era, and that the parallel reading and writing of compressed data will be supported.  Similarly we expect that in the \dword{dune} era, new organization and representations of tree'd data structures (e.g. n-tuple or TTree like structures) will be available through the \dword{rntuple} system\cite{Blomer:2020usr, ROOTTeam:2020jal}. It is expected that \dword{rntuple} will provide columnar data access and performance similar to \dword{hdf5}. Indeed, the \dword{lhc} experiments are expected to migrate to the \dword{rntuple} format on the timescale of \dword{dune}.
As a result of these features of the format, and the use of the format across \dword{hep}, we expect to support the \dword{root} format as part of our computing model. 

One concern that arose during our format evaluation was the availability of a streaming option for \dword{hdf5}. Our large-scale offline processing is most efficient with a mix of both streaming and direct copies of input data.   Streaming also allows any \dword{dune} site to access centrally stored data transparently.  For this reason \dword{hdf5} streaming is very desirable.  We have successfully adapted the \dword{dune} \dword{larsoft} framework to read  local \dword{hdf5} input and very recently have demonstrated streaming via \dword{xrootd} in test mode.  

In addition to the \dword{root} and \dword{hdf5} data formats, we expect that the \dword{dune} data representations will need to be flexible and adapt to changing technologies and evolutions in the data science and physics communities.  For this reason we expect to allow for the addition of other data representations and have placed requirements on the software frameworks used by \dword{dune}, as discussed in chapter~\ref{ch:fworks}, to support multiple data formats and I/O layers, as well as requirements on the data management and cataloging systems to support multiple data formats.

%These access patterns are complimentary to the in     fundamental types, such as integers and floating-point numbers, along with metadata such as group names and dataset names.  \dword{root} files support a much more complex structure.  \dword{root} provides mechanisms for saving and retrieving data in C\raisebox{1pt}{++} classes, provides for automated conversion of data on reading when classes change from one version of user code to the next (schema evolution), and provides structures like trees. \dword{root} also supports integrated streaming through \dword{xrootd}. 

%\dword{hdf5}'s uniformity of data representation has both advantages and disadvantages. For simple, repetitive data structures, such as the raw waveforms generated by \dword{dune}, the simplicity of \dword{hdf5} is useful.  Many third party machine learning algorithms use \dword{hdf5} and users are already converting \dword{root} to \dword{hdf5} when using these algorithms.  However, for interaction reconstruction, \dword{root}'s features for processing complex data objects optimized for \dword{hep} and integration with the \dword{art} framework are useful and convenient.  The current \dword{dune} processing chain uses both formats, and the choice has been optimized for each purpose.

%In particular, the \dword{protoii} \dword{daq} system now writes raw data in \dword{hdf5} format. The utilization of \dword{hdf5} libraries within the \dword{daq} software allows for multiple processes to write to the same output file asynchronously. This ability is very advantageous when acquiring data from multiple \dword{fd} modules or multiple \dword{nd} subdetectors. The implementation of \dword{hdf5} is being tested %anne within  \dword{pdcoldbox} 
%in the \dword{np02} \coldbox at the \dword{cern} with a small prototype of the %anne \dword{pdvd} 
%\dword{spvd} \dword{protodune} readout systems. 
%\todo{anne: \dword{pdcoldbox} and \dword{pdvd}  aren't standard per the \dword{spvd} CDR }

%With the advent of the new \dword{daq} format, \dword{hdf5} formatted data have been integrated successfully with the \dword{art} and \dword{larsoft} frameworks in a manner that allows improved memory management for large data records.  The  input \dword{hdf5} record is parsed and pointers to large data arrays are then made available, sequentially, to the \dword{art}/\dword{root} event processing framework for further processing. Development work is needed to understand if implementation of streaming for \dword{hdf5} data over the network is possible or needed for efficient distributed processing of raw data files.


%\subsection{Future \hideme{Laycock - check for redundancy}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTE:   The Blomer paper is very old, and actual data that was
% examined was down right tiny (only 1.5 GB of data!!!!) even the
% event data representations were itty bitty -- 26 words per event
% and yes WORDs not doubles.  It is not very representative of 
% what the DUNE data will look like, or what the analysis
% structure and reads/write will look like.
%
% The point is -- we need to be VERY careful about citing this
% paper because while it may claim that ROOT is great, the
% context under which the tests were run are not what we as
% DUNE will do.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Happy to nix this completely
%The results demonstrated that the effort to optimize the \dword{root} I/O system has been an excellent investment as it out-performed all other data formats, including \dword{hdf5}, except for a few corner cases
%\footnote{Pure serialization using Google's Protobuf library was seen to be faster as it does not manage columns.}.


%In the near future, the next generation \dword{root} data format, called RNtuple 
%\cite{Blomer:2020usr, ROOTTeam:2020jal} promises even more I/O optimization for \dword{hep} use cases.  The \dword{hsf} input to the high luminosity \dword{lhc} computing review \cite{HEPSoftwareFoundation:2020daq} stressed that, particularly for I/O bound analysis, it's unlikely that a drop-in replacement for \dword{root} will be found.
%Indeed, the \dword{lhc} experiments are expected to migrate to the RNtuple format on the timescale of \dword{dune}.  Using the same data format as the \dword{lhc} experiments would have clear advantages for \dword{dune}, particularly with regard to enabling interoperability of various elements of the software stack. \dword{dune} plans to work together with the \dword{root} team to investigate potential solutions for the issues seen with \dword{protodune} data.

%Despite the success of \dword{root}, there are other popular data formats like \dword{hdf5} that \dword{dune} will continue to investigate.  The astronomy community produces data not dissimilar to \dword{dune} and the Advanced Scientific Data Format (ASDF) addresses some of the limitations of \dword{hdf5} while retaining its convenience. Another key consideration of data formats is compatibility with the tools of the wider data science community, particularly for machine learning applications, and \dword{dune} will need to follow developments here too.  A common solution to this compatibility issue in \dword{hep} experiments is to use an interface layer that can mitigate the choice of persistent data format with some performance cost that may often be negligible in many use cases.  The scikit-hep project\cite{Rodrigues:2019nct} provides a python ecosystem to read \dword{root} persistent data formats and convert them to native data science formats.

Given the vast variation in scale and data access patterns that \dword{dune} computing needs to support, it is likely that most if not all of these options, and additional as-yet-unknown options, will have a role in creating the most effective suite of solutions to support the physics program. Detailed studies of the \dword{protodune} I \& II datasets will be needed to make cost-benefit analyses of the various options and guide the analysis model.  A final consideration, and by no means the least important, will be the ease of use of the analysis model for physicists to maximize their analysis output.

\end{document}