\documentclass[../main-v1.tex]{subfiles}
\begin{document}
\chapter{Data Formats}
\label{ch:format}

\section{Introduction \hideme{Schellman - draft}} 

\dword{dune} data are stored and processed in a variety of forms.   They are characterized by their \dword{datatier} and their data format.  The data format refers to the low level format of the data, for example text, root or \dword{hdf5} while the data tier is linked to the processing transformations that produced the file.  

\section{Data tiers \hideme{Schellman - add a discussion here}}

As data are processed, they evolve through a series of \dwords{datatier}.  The \dword{datatier} concept was first adopted by the D0 and CDF collaborations two decades ago and is an important component in the definition of datasets in the data catalog.   Examples of \dwords{datatier} are raw, hit-reconstructed, full-reconstructed, reco-recalibrated and root-tuple. 
Reconstruction algorithms transform files from one \dword{datatier} to another in a defined way and files in a given data tier can be expected to contain similar detector and physics information.   
Policies for reading and storage can be defined at the data-tier level. 
Table \ref{table:format1} summarizes some of the more common formats and their sizes. 

\begin{dunetable}
[Data Tier Summary]
{l l r c l l} 
 {table:format1}
 {Example data tiers.  The sizes given are the total on tape, not the amount present on disk under the retention policies.}
Data Tier & Produced by &current size & Copies & Disk Lifetime  & Tape Lifetime \\ [0.5ex] 
raw&DAQ&5.2 PB&2&few months&$>20$years\\
detector-simulated&geant4+detsim&4.1 PB&1&transient&5-10 years\\
hit-reconstructed&hit finding&0.2 PB&1&transient&5-10 years\\
full-reconstructed (MC)&pattern recognition&5.7 PB&1&2 years&5-10 years\\
full-reconstructed (data)&pattern recognition&3.4 PB&1&2 years&5-10 years\\
root-tuple&data reductions&24 TB&1
&2-3 years&5-10 years\\
\end{dunetable}

Within a data tier, data can have different formats.  An example is the use of \dword{root} formats for ProtoDUNE Run I raw data and \dword{hdf5} format for ProtoDUNE Run II.

\section{I/O formats \hideme{Schellman/Bashyal - draft}}
\dword{dune} has historically used the \dword{root} data format throughout the processing chain.  \dword{root} provides data objects and methods that have been optimized for \dword{hep} applications.  The \dword{larsoft} framework is \dword{root}-compatible and \dword{root} was used for the full processing chain for \dword{protodune} Run 1. 

However, \dword{hep} increasingly uses   machine learning methods that rely on non-\dword{hep} data formats, notably the \dword{hdf5} format.  \dword{hdf5} is designed to save data as arrays of fundamental types, such as integers and floating-point numbers, along with metadata such as group names and dataset names.  \dword{root} files have much more structure.  \dword{root} provides mechanisms for saving and retrieving data in C\raisebox{1pt}{++} classes, provides for automated conversion of data on reading when classes change from one version of user code to the next (schema evolution), and provides structures like trees. \dword{hdf5}'s uniformity of data representation has both advantages and disadvantages. For simple, repetitive data structures, such as the raw waveforms generated by \dword{dune}, the simplicity of \dword{hdf5} is useful.  For interaction reconstruction, \dword{root}'s features for processing complex data objects optimized for \dword{hep} are useful and convenient.  The current \dword{dune} processing chain uses both formats, and the choice has been optimized for each purpose.

In particular, the \dword{protoii} \dword{daq} system now writes raw data in \dword{hdf5} format. The utilization of \dword{hdf5} libraries within the \dword{daq} software allows for multiple processes to write to the same output file asynchronously. This ability is very advantageous when acquiring data from multiple \dword{fd} modules or multiple \dword{nd} subdetectors. The implementation of \dword{hdf5} is being tested %anne within  \dword{pdcoldbox} 
in the \dword{np02} \coldbox at the \dword{cern} with a small prototype of the %anne \dword{pdvd} 
\dword{spvd} \dword{protodune} readout systems. 
\todo{anne: \dword{pdcoldbox} and \dword{pdvd}  aren't standard per the \dword{spvd} CDR }

The \dword{hdf5} format has been integrated successfully with the \dword{art} and \dword{larsoft} frameworks in a manner that allows improved memory management for large data records.  The  input \dword{hdf5} record is parsed and pointers to large data arrays are then made available, sequentially, to the \dword{art}/\dword{root} event processing framework for further processing. Development work to understand if streaming of \dword{hdf5} data over the network is possible or needed for efficient distributed processing of raw data files.

\subsection{Future \hideme{Laycock/Bashyal - need to extend}}
In the near future, the next generation \dword{root} data format, called RNtuple \todo{(NEEDS REFERENCE)} promises even more I/O optimization for \dword{hep} use cases and the \dword{lhc} experiments are expected to migrate to this format on the timescale of \dword{dune}.  Using the same data format as the \dword{lhc} experiments would have clear advantages for \dword{dune}, particularly with regard to enabling interoperability of various elements of the software stack.

\end{document}